# **1.SQL语句的优化**

## **1.尽量避免使用子查询**

## **2.用IN来替换OR**

## **3.读取适当的记录LIMIT M,N，而不要读多余的记录**

## **4.禁止不必要的Order By排序**

## **5.总和查询可以禁止排重用union all**

## **6.避免随机取记录**

## **7.将多次插入换成批量Insert插入**

## **8.只返回必要的列，用具体的字段列表代替 select \* 语句**

## **9.区分in和exists**

## **10.优化Group By语句**

## **11.尽量使用数字型字段**

## **12.优化Join语句**

# 2.MySQL索引使用的注意事项

## 1.不要在列上使用函数和进行运算

## 2.尽量避免使用 != 或 not in或 <> 等否定操作符

## 3.尽量避免使用 or 来连接条件

## 4.联合索引由于多个单列索引

## 5.满足复合索引的最左前缀原则

## 6.覆盖索引的好处

## 7.范围查询的右边索引失效

## 8.索引不会包含有NULL值的列

## 9.隐式转换的影响

## 10.like 查询以%开头

## 11.全值匹配

## 12.is null可以使用上索引，is not null不行

## 13.**数据库和表的字符集统一使用utf8mb4**

# 3.MySQL 遇到过死锁问题吗，你是如何解决的

死锁是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程。

## 死锁形成的四大条件

### 1. 互斥条件

在一段时间内，计算机中的某个资源只能被一个进程占用。此时，如果其他进程请求该资源，则只能等待。

### 2. 不可剥夺条件

某个进程获得的资源在使用完毕之前，不能被其他进程强行夺走，只能由获得资源的进程主动释放。

### 3. 请求与保持条件

进程已经获得了至少一个资源，又要请求其他资源，但请求的资源已经被其他进程占有，此时请求的进程就会被阻塞，并且不会释放自己已获得的资源。

### 4. 循环等待条件

系统中的进程之间相互等待，同时各自占用的资源又会被下一个进程所请求。例如有进程A、进程B和进程C三个进程，进程A请求的资源被进程B占用，进程B请求的资源被进程C占用，进程C请求的资源被进程A占用，于是形成了循环等待条件，如图1-7所示。

[![img](https://s2.51cto.com/oss/202110/20/bd9a3d963f3dd3648368aead258bc556.jpg)](https://s2.51cto.com/oss/202110/20/bd9a3d963f3dd3648368aead258bc556.jpg)

▲图1-7 死锁的循环等待条件

需要注意的是，只有4个必要条件都满足时，才会发生死锁。

处理死锁有4种方法，分别为预防死锁、避免死锁、检测死锁和解除死锁，如图1-8所示。

[![img](https://s4.51cto.com/oss/202110/20/89c460f0989f488c30d71c29a07b65e5.jpg)](https://s4.51cto.com/oss/202110/20/89c460f0989f488c30d71c29a07b65e5.jpg)

## 处理死锁的方法

### 预防死锁

处理死锁最直接的方法就是破坏造成死锁的4个必要条件中的一个或多个，以防止死锁的发生。

### 避免死锁

在系统资源的分配过程中，使用某种策略或者方法防止系统进入不安全状态，从而避免死锁的发生。

### 检测死锁

这种方法允许系统在运行过程中发生死锁，但是能够检测死锁的发生，并采取适当的措施清除死锁。

### 解除死锁

当检测出死锁后，采用适当的策略和方法将进程从死锁状态解脱出来。

# 4.说说分库与分表的设计

## 1.基本思想

Sharding的基本思想就要把一个数据库切分成多个部分放到不同的数据库(server)上，从而缓解单一数据库的性能问题。

- - 
- 对于海量数据的数据库，如果是因为表多而数据多，这时候适合使用垂直切分，即把关系紧密（比如同一模块）的表切分出来放在一个server上。
- 如果表并不多，但每张表的数据非常多，这时候适合水平切分，即把表的数据按某种规则（比如按ID散列）切分到多个数据库(server)上。
  - 

当然，现实中更多是这两种情况混杂在一起，这时候需要根据实际情况做出选择，也可能会综合使用垂直与水平切分，从而将原有数据库切分成类似矩阵一样可以无限扩充的数据库(server)阵列。

需要特别说明的是：

　　当同时进行垂直和水平切分时，切分策略会发生一些微妙的变化。比如：在只考虑垂直切分的时候，被划分到一起的表之间可以保持任意的关联关系，因此你可以按“功能模块”划分表格，但是一旦引入水平切分之后，表间关联关系就会受到很大的制约，通常只能允许一个主表（以该表ID进行散列的表）和其多个次表之间保留关联关系，也就是说：当同时进行垂直和水平切分时，在垂直方向上的切分将不再以“功能模块”进行划分，而是需要更加细粒度的垂直切分，而这个粒度与领域驱动设计中的“聚合”概念不谋而合，甚至可以说是完全一致，每个shard的主表正是一个聚合中的聚合根！这样切分下来你会发现数据库分被切分地过于分散了（shard的数量会比较多，但是shard里的表却不多），为了避免管理过多的数据源，充分利用每一个数据库服务器的资源，可以考虑将业务上相近，并且具有相近数据增长速率（主表数据量在同一数量级上）的两个或多个shard放到同一个数据源里，每个shard依然是独立的，它们有各自的主表，并使用各自主表ID进行散列，不同的只是它们的散列取模（即节点数量）必需是一致的.

## 2.常用的分库分表中间件

### 1.简单易用的组件

- - 

- [ 当当sharding-jdbc](https://github.com/dangdangdotcom/sharding-jdbc)

- 

   蘑菇街TSharding

  

  - 

### 2.强悍重量级的中间件

- [ sharding](https://github.com/go-pg/sharding)
- [ TDDL Smart Client的方式（淘宝）](https://github.com/alibaba/tb_tddl)
- [ Atlas(Qihoo 360)](https://github.com/Qihoo360/Atlas)
- [ alibaba.cobar(是阿里巴巴（B2B）部门开发)](https://github.com/alibaba/cobar)
- [ MyCAT（基于阿里开源的Cobar产品而研发）](http://www.mycat.org.cn/)
- [ Oceanus(58同城数据库中间件)](https://github.com/58code/Oceanus)
- OneProxy(支付宝首席架构师楼方鑫开发)
-  vitess（谷歌开发的数据库中间件）

### 3.分库分表需要解决的问题

#### 1.事务问题

解决事务问题目前有两种可行的方案：分布式事务和通过应用程序与数据库共同控制实现事务下面对两套方案进行一个简单的对比。

##### 方案一：使用分布式事务

优点：交由数据库管理，简单有效

缺点：性能代价高，特别是shard越来越多时

##### 方案二：由应用程序和数据库共同控制

原理：将一个跨多个数据库的分布式事务分拆成多个仅处 于单个数据库上面的小事务，并通过应用程序来总控 各个小事务。

优点：性能上有优势

缺点：需要应用程序在事务控制上做灵活设计。如果使用 了spring的事务管理，改动起来会面临一定的困难。

#### 2.跨节点Join的问题

只要是进行切分，跨节点Join的问题是不可避免的。但是良好的设计和切分却可以减少此类情况的发生。解决这一问题的普遍做法是分两次查询实现。在第一次查询的结果集中找出关联数据的id,根据这些id发起第二次请求得到关联数据。

#### 3.跨节点的count,order by,group by以及聚合函数问题

这些是一类问题，因为它们都需要基于全部数据集合进行计算。多数的代理都不会自动处理合并工作。解决方案：与解决跨节点join问题的类似，分别在各个节点上得到结果后在应用程序端进行合并。和join不同的是每个结点的查询可以并行执行，因此很多时候它的速度要比单一大表快很多。但如果结果集很大，对应用程序内存的消耗是一个问题。

#### 4.数据迁移，容量规划，扩容等问题

来自淘宝综合业务平台团队，它利用对2的倍数取余具有向前兼容的特性（如对4取余得1的数对2取余也是1）来分配数据，避免了行级别的数据迁移，但是依然需要进行表级别的迁移，同时对扩容规模和分表数量都有限制。总得来说，这些方案都不是十分的理想，多多少少都存在一些缺点，这也从一个侧面反映出了Sharding扩容的难度。

#### 5.事务

##### (1)分布式事务

```
参考： [关于分布式事务、两阶段提交、一阶段提交、Best Efforts 1PC模式和事务补偿机制的研究]1.
```

优点

1. 基于两阶段提交，最大限度地保证了跨数据库操作的“原子性”，是分布式系统下最严格的事务实现方式。
2. 实现简单，工作量小。由于多数应用服务器以及一些独立的分布式事务协调器做了大量的封装工作，使得项目中引入分布式事务的难度和工作量基本上可以忽略不计。

- 缺点

1. 系统“水平”伸缩的死敌。基于两阶段提交的分布式事务在提交事务时需要在多个节点之间进行协调,最大限度地推后了提交事务的时间点，客观上延长了事务的执行时间，这会导致事务在访问共享资源时发生冲突和死锁的概率增高，随着数据库节点的增多，这种趋势会越来越严重，从而成为系统在数据库层面上水平伸缩的"枷锁"， 这是很多Sharding系统不采用分布式事务的主要原因。

- 

##### (2)基于Best Efforts 1PC模式的事务

参考[ spring-data-neo4j](https://github.com/SpringSource/spring-data-graph/blob/master/spring-data-neo4j/src/main/java/org/springframework/data/neo4j/transaction/ChainedTransactionManager.java)的实现。鉴于Best Efforts 1PC模式的性能优势，以及相对简单的实现方式，它被大多数的sharding框架和项目采用

##### (3)事务补偿（幂等值）

对于那些对性能要求很高，但对一致性要求并不高的系统，往往并不苛求系统的实时一致性，只要在一个允许的时间周期内达到最终一致性即可，这使得事务补偿机制成为一种可行的方案。事务补偿机制最初被提出是在“长事务”的处理中，但是对于分布式系统确保一致性也有很好的参考意义。笼统地讲，与事务在执行中发生错误后立即回滚的方式不同，事务补偿是一种事后检查并补救的措施，它只期望在一个容许时间周期内得到最终一致的结果就可以了。事务补偿的实现与系统业务紧密相关，并没有一种标准的处理方式。一些常见的实现方式有：对数据进行对帐检查;基于日志进行比对;定期同标准数据来源进行同步，等等。

#### 6、ID问题

一旦数据库被切分到多个物理结点上，我们将不能再依赖数据库自身的主键生成机制。一方面，某个分区数据库自生成的ID无法保证在全局上是唯一的；另一方面，应用程序在插入数据之前需要先获得ID,以便进行SQL路由.

一些常见的主键生成策略：

##### (1)UUID

使用UUID作主键是最简单的方案，但是缺点也是非常明显的。由于UUID非常的长，除占用大量存储空间外，最主要的问题是在索引上，在建立索引和基于索引进行查询时都存在性能问题。

##### (2)结合数据库维护一个Sequence表

此方案的思路也很简单，在数据库中建立一个Sequence表，表的结构类似于：

```
CREATE TABLE `SEQUENCE` (  
    `table_name` varchar(18) NOT NULL, `nextid` bigint(20) NOT NULL, PRIMARY KEY (`table_name`) ) ENGINE=InnoDB1.2.
```

每当需要为某个表的新纪录生成ID时就从Sequence表中取出对应表的nextid,并将nextid的值加1后更新到数据库中以备下次使用。此方案也较简单，但缺点同样明显：由于所有插入任何都需要访问该表，该表很容易成为系统性能瓶颈，同时它也存在单点问题，一旦该表数据库失效，整个应用程序将无法工作。有人提出使用Master-Slave进行主从同步，但这也只能解决单点问题，并不能解决读写比为1:1的访问压力问题。

##### (3)[ Twitter的分布式自增ID算法Snowflake](http://blog.sina.com.cn/s/blog_6b7c2e660102vbi2.html)

在分布式系统中，需要生成全局UID的场合还是比较多的，twitter的snowflake解决了这种需求，实现也还是很简单的，除去配置信息，核心代码就是毫秒级时间41位 机器ID 10位 毫秒内序列12位。

```
* 10---0000000000 0000000000 0000000000 0000000000 0 --- 00000 ---00000 ---0000000000001.
```

在上面的字符串中，第一位为未使用（实际上也可作为long的符号位），接下来的41位为毫秒级时间，然后5位datacenter标识位，5位机器ID（并不算标识符，实际是为线程标识），然后12位该毫秒内的当前毫秒内的计数，加起来刚好64位，为一个Long型。

这样的好处是，整体上按照时间自增排序，并且整个分布式系统内不会产生ID碰撞（由datacenter和机器ID作区分），

并且效率较高，经测试，snowflake每秒能够产生26万ID左右，完全满足需要。

#### 7.跨分片的排序分页

一般来讲，分页时需要按照指定字段进行排序。当排序字段就是分片字段的时候，我们通过分片规则可以比较容易定位到指定的分片，而当排序字段非分片字段的时候，情况就会变得比较复杂了。为了最终结果的准确性，我们需要在不同的分片节点中将数据进行排序并返回，并将不同分片返回的结果集进行汇总和再次排序，最后再返回给用户。如下图所示：

![设计----【分库、分表】分库分表的基本思想_分布式事务](https://s2.51cto.com/images/blog/202201/17095715_61e4ccfbc490240224.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=/format,webp/resize,m_fixed,w_1184)

上面图中所描述的只是最简单的一种情况（取第一页数据），看起来对性能的影响并不大。但是，如果想取出第10页数据，情况又将变得复杂很多，如下图所示：

![设计----【分库、分表】分库分表的基本思想_分布式事务_02](https://s2.51cto.com/images/blog/202201/17095715_61e4ccfbd61de87024.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=/format,webp/resize,m_fixed,w_1184)

有些读者可能并不太理解，为什么不能像获取第一页数据那样简单处理（排序取出前10条再合并、排序）。其实并不难理解，因为各分片节点中的数据可能是随机的，为了排序的准确性，必须把所有分片节点的前N页数据都排序好后做合并，最后再进行整体的排序。很显然，这样的操作是比较消耗资源的，用户越往后翻页，系统性能将会越差。

那如何解决分库情况下的分页问题呢？有以下几种办法：

如果是在前台应用提供分页，则限定用户只能看前面n页，这个限制在业务上也是合理的，一般看后面的分页意义不大（如果一定要看，可以要求用户缩小范围重新查询）。

如果是后台批处理任务要求分批获取数据，则可以加大page size，比如每次获取5000条记录，有效减少分页数（当然离线访问一般走备库，避免冲击主库）。

分库设计时，一般还有配套大数据平台汇总所有分库的记录，有些分页查询可以考虑走大数据平台。

#### 8.分库策略

分库维度确定后，如何把记录分到各个库里呢?

一般有两种方式：

- - 
- 根据数值范围，比如用户Id为1-9999的记录分到第一个库，10000-20000的分到第二个库，以此类推。
- 根据数值取模，比如用户Id mod n，余数为0的记录放到第一个库，余数为1的放到第二个库，以此类推。
  - 

优劣比较：

- - 
- 评价指标按照范围分库按照Mod分库
- 库数量前期数目比较小，可以随用户/业务按需增长前期即根据mode因子确定库数量，数目一般比较大
- 访问性能前期库数量小，全库查询消耗资源少，单库查询性能略差前期库数量大，全库查询消耗资源多，单库查询性能略好
- 调整库数量比较容易，一般只需为新用户增加库，老库拆分也只影响单个库困难，改变mod因子导致数据在所有库之间迁移
- 数据热点新旧用户购物频率有差异，有数据热点问题新旧用户均匀到分布到各个库，无热点
  - 

实践中，为了处理简单，选择mod分库的比较多。同时二次分库时，为了数据迁移方便，一般是按倍数增加，比如初始4个库，二次分裂为8个，再16个。这样对于某个库的数据，一半数据移到新库，剩余不动，对比每次只增加一个库，所有数据都要大规模变动。

补充下，mod分库一般每个库记录数比较均匀，但也有些数据库，存在超级Id，这些Id的记录远远超过其他Id，比如在广告场景下，某个大广告主的广告数可能占总体很大比例。如果按照广告主Id取模分库，某些库的记录数会特别多，对于这些超级Id，需要提供单独库来存储记录。

#### 9.分库数量

分库数量首先和单库能处理的记录数有关，一般来说，Mysql 单库超过5000万条记录，Oracle单库超过1亿条记录，DB压力就很大(当然处理能力和字段数量/访问模式/记录长度有进一步关系)。

在满足上述前提下，如果分库数量少，达不到分散存储和减轻DB性能压力的目的；如果分库的数量多，好处是每个库记录少，单库访问性能好，但对于跨多个库的访问，应用程序需要访问多个库，如果是并发模式，要消耗宝贵的线程资源；如果是串行模式，执行时间会急剧增加。

最后分库数量还直接影响硬件的投入，一般每个分库跑在单独物理机上，多一个库意味多一台设备。所以具体分多少个库，要综合评估，一般初次分库建议分4-8个库。

#### 10.路由透明

分库从某种意义上来说，意味着DB schema改变了，必然影响应用，但这种改变和业务无关，所以要尽量保证分库对应用代码透明，分库逻辑尽量在数据访问层处理。当然完全做到这一点很困难，具体哪些应该由DAL负责，哪些由应用负责，这里有一些建议：

对于单库访问，比如查询条件指定用户Id，则该SQL只需访问特定库。此时应该由DAL层自动路由到特定库，当库二次分裂时，也只要修改mod 因子，应用代码不受影响。

对于简单的多库查询，DAL负责汇总各个数据库返回的记录，此时仍对上层应用透明。

#### 11.使用框架还是自主研发

目前市面上的分库分表中间件相对较多，其中基于代理方式的有MySQL Proxy和Amoeba，基于Hibernate框架的是Hibernate Shards，基于jdbc的有[ 当当sharding-jdbc](https://github.com/dangdangdotcom/sharding-jdbc)，基于mybatis的类似maven插件式的有蘑菇街的[ 蘑菇街TSharding](https://github.com/baihui212/tsharding)，通过重写spring的ibatis template类是Cobar Client，这些框架各有各的优势与短板，架构师可以在深入调研之后结合项目的实际情况进行选择，但是总的来说，我个人对于框架的选择是持谨慎态度的。一方面多数框架缺乏成功案例的验证，其成熟性与稳定性值得怀疑。另一方面，一些从成功商业产品开源出框架（如阿里和淘宝的一些开源项目）是否适合你的项目是需要架构师深入调研分析的。当然，最终的选择一定是基于项目特点、团队状况、技术门槛和学习成本等综合因素考量确定的。

# 5.InnoDB与MyISAM的区别

## 事物支持

inoodb支持事物，myisam不支持

## 外键支持

inoodb支持外键,myisam不支持

## 锁的粒度

inoodb支持行级锁 myisam支持表级锁

## 索引结构

 innodb索引即存放索引也存放数据，myisam只存放索引

## 表结构

表结构不一样  inoodb结构 8.0 idb  myisam frm.myi,myd

# 6.数据库索引的原理，为什么要用 B+树，为什么不用二叉树？

首先，B+树查询效率更稳定。因为B+树每次只有访问到叶子节点才能找到对应的数据，而在B树中，非叶子节
点也会存储数据，这样就会造成查询效率不稳定的情况，有时候访问到了非叶子节点就可以找到关键字，而有时
需要访问到叶子节点才能找到关键字。

其次，B+树的查询效率更高。这是因为通常B+树比B树更矮胖（阶数更大，深度更低），查询所需要的磁盘
io也会更少。同样的磁盘页大小，B+树可以存储更多的节点关键字。

不仅是对单个关键字的查询上，在查询范围上，B+树的效率也比B树高。这是因为所有关键字都出现在B+树的
叶子节点中，叶子节点之间会有指针，数据又是递增的，这使得我们范围查找可以通过指针连接查找。而在B树
中则需要通过中序遍历才能完成查询范围的查找，效率要低很多

InnoDB 存储引擎支持以下几种常见的索引： B+树索引、全文索引、哈希索引 而 B+树索引最为常见，可以分为聚集索引和非聚集索引。非聚集索引也可以叫做辅助索引，二级索引。

# 7.聚集索引与非聚集索引的区别

两种索引相同点： 内部都是 B+ 树，高度平衡，叶子节点存放着所有的数据。

不同点： 聚集索引的叶子节点存放是一整行的信息。 聚集索引一个表只能有一个，而非聚集索引一个表可以存在多个。 聚集索引存储记录是物理上连续存在，而非聚集索引是逻辑上的连续，物理存储并不连续。 聚集索引查询数据速度快，插入数据速度慢;非聚集索引反之。 聚集索引范围查询快。

聚集索引： InnoDB 存储引擎表是索引组织表，表种数据按照主键顺序存放，而聚集索引就是按照每张表的主键构造一颗 B+ 数，同时叶子节点中存放的就是整张表的行记录数据，也将聚集索引的叶子节点称为数据页。 每张表只能拥有一个聚集索引。 查询优化器倾向于采用聚集索引。

非聚集索引： 叶子节点不包含记录的全部数据。 叶子节点中索引行中还包含了一个书签，用来告诉 InnoDB 存储引擎在哪里可以找到与索引相应的行数据。 这个书签就是相应的行数据的聚集索引键。 可以有多个非聚集索引。 使用非聚集索引来寻找数据时，通过叶级别的指针获得指向主键索引的主键，再通过主键索引找到一个完整的行记录。

# 8.limit 1000000 加载很慢的话，你是怎么解决的呢

方案一：如果id是连续的，可以这样，返回上次查询的最大记录(偏移量)，再往下limit
select id，name from employee where id>1000000 limit 10;

复制代码方案二：在业务允许的情况下限制页数：
建议跟业务讨论，有没有必要查这么后的分页啦。因为绝大多数用户都不会往后翻太多页。

方案三：order by + 索引（id为索引）
select id，name from employee order by id  limit 1000000，10

# 9.如何选择合适的分布式主键方案呢？

数据库自增长序列或字段。

UUID。

Redis生成ID

Twitter的snowflake算法

利用zookeeper生成唯一ID

MongoDB的ObjectId

# 10.事务的隔离级别有哪些？MySQL的默认隔离级别是什么？

MySQL 默认的事务隔离级别是可重复读(REPEATABLE READ)，这 4 种隔离级别的说明如下

## 1.READ UNCOMMITTED

读未提交，也叫未提交读，该隔离级别的事务可以看到其他事务中未提交的数据。该隔离级别因为可以读取到其他事务中未提交的数据，而未提交的数据可能会发生回滚，因此我们把该级别读取到的数据称之为脏数据，把这个问题称之为脏读。

## 2.READ COMMITTED

读已提交，也叫提交读，该隔离级别的事务能读取到已经提交事务的数据，因此它不会有脏读问题。但由于在事务的执行中可以读取到其他事务提交的结果，所以在不同时间的相同 SQL 查询中，可能会得到不同的结果，这种现象叫做不可重复读。

## 3.REPEATABLE READ

可重复读，是 MySQL 的默认事务隔离级别，它能确保同一事务多次查询的结果一致。但也会有新的问题，比如此级别的事务正在执行时，另一个事务成功的插入了某条数据，但因为它每次查询的结果都是一样的，所以会导致查询不到这条数据，自己重复插入时又失败(因为唯一约束的原因)。明明在事务中查询不到这条信息，但自己就是插入不进去，这就叫幻读 (Phantom Read)。

## 4.SERIALIZABLE

序列化，事务最高隔离级别，它会强制事务排序，使之不会发生冲突，从而解决了脏读、不可重复读和幻读问题，但因为执行效率低，所以真正使用的场景并不多。

简单总结一下，MySQL 的 4 种事务隔离级别对应脏读、不可重复读和幻读的关系如下：

[![img](https://s6.51cto.com/oss/202201/03/03f9024f847f83348a961b9d7eef0687.png)](https://s6.51cto.com/oss/202201/03/03f9024f847f83348a961b9d7eef0687.png)

只看以上概念会比较抽象，接下来，咱们一步步通过执行的结果来理解这几种隔离级别的区别。

# 11.什么是幻读，脏读，不可重复读呢

## 脏写

一个事物修改了另一个未提交事物所修改的字段

## 脏读

一个事物读物到了另一个未提交事物所做的修改

## 不可重复读

一个未提交事物执行同一个查询，因为另一个事物的不断修改操作，造成每次读取的数据不一致

## 幻读

一个未提交事物执行同一个查询，因为另一个事物数据的插入，导致读取的数据的行数不一致

# 12.在高并发情况下，如何做到安全的修改同一行数据

## 1、使用悲观锁

悲观锁本质是当前只有一个线程执行操作，排斥外部请求的修改。遇到加锁的状态，就必须等待。结束了唤醒其他线程进行处理。虽然此方案的确解决了数据安全的问题，但是，我们的场景是“高并发”。也就是说，会很多这样的修改请求，每个请求都需要等待“锁”，某些线程可能永远都没有机会抢到这个“锁”，这种请求就会死在那里。同时，这种请求会很多，瞬间增大系统的平均响应时间，结果是可用连接数被耗尽，系统陷入异常。

## 2、FIFO（First Input First Output，先进先出）缓存队列思路

直接将请求放入队列中，就不会导致某些请求永远获取不到锁。看到这里，是不是有点强行将多线程变成单线程的感觉哈。



然后，我们现在解决了锁的问题，全部请求采用“先进先出”的队列方式来处理。那么新的问题来了，高并发的场景下，因为请求很多，很可能一瞬间将队列内存“撑爆”，然后系统又陷入到了异常状态。或者设计一个极大的内存队列，也是一种方案，但是，系统处理完一个队列内请求的速度根本无法和疯狂涌入队列中的数目相比。也就是说，队列内的请求会越积累越多，最终Web系统平均响应时间还是会大幅下降，系统还是陷入异常。

## 3、使用乐观锁

这个时候，我们就可以讨论一下“乐观锁”的思路了。乐观锁，是相对于“悲观锁”采用更为宽松的加锁机制，大都是采用带版本号（Version）更新。实现就是，这个数据所有请求都有资格去修改，但会获得一个该数据的版本号，只有版本号符合的才能更新成功，其他的返回抢购失败。这样的话，我们就不需要考虑队列的问题，不过，它会增大CPU的计算开销。但是，综合来说，这是一个比较好的解决方案。

# 13.数据库的乐观锁和悲观锁

## **乐观锁**

乐观锁（Optimistic Locking）认为对同一数据的并发操作不会总发生，属于小概率事件，不用每次都对数据上锁，也就是不采用数据库自身的锁机制，而是通过程序来实现。在程序上，我们可以采用版本号机制或者时间戳机制实现。

- 乐观锁的版本号机制
  在表中设计一个版本字段 version，第一次读的时候，会获取 version 字段的取值。然后对数据进行更新或删除操作时，会执行UPDATE ... SET version=version+1 WHERE version=version。此时如果已经有事务对这条数据进行了更改，修改就不会成功。
- 乐观锁的时间戳机制
  时间戳和版本号机制一样，也是在更新提交的时候，将当前数据的时间戳和更新之前取得的时间戳进行比较，如果两者一致则更新成功，否则就是版本冲突。

## **悲观锁**

悲观锁（Pessimistic Locking）也是一种思想，对数据被其他事务的修改持保守态度，会通过数据库自身的锁机制来实现，从而保证数据操作的排它性。

适用场景

- 乐观锁适合读操作多的场景，相对来说写的操作比较少。它的优点在于程序实现，不存在死锁问题，不过适用场景也会相对乐观，因为它阻止不了除了程序以外的数据库操作。
- 悲观锁适合写操作多的场景，因为写的操作具有排它性。采用悲观锁的方式，可以在数据库层面阻止其他事务对该数据的操作权限，防止读 - 写和写 - 写的冲突。

#  14.select for update有什么含义，会锁表还是锁行还是其他？

因此在RC隔离级别下，如果条件是主键，那么select......for update加的就是两把锁，一把IX意向排他锁（不影响插入），一把对应主键的X排他锁（行锁，会锁住主键索引那一行）

如果是 RR（可重复读）的数据库隔离级别，select...for update的查询条件是主键的话，加的又是什么锁呢？

发现一共加了两把锁，分别是：IX意向排他锁（表锁）、一把X排他锁（行锁，对应主键的X锁）。

根据前面的实验结果，我们其实可以推测得出来了，应该跟RC隔离级别一样，会加两把锁：一把IX意向排他锁（表锁，不影响插入），一把对应主键的X排他锁（行锁，影响对应主键那一行的插入）。

如果查询条件，没有命中数据库表的记录，又加什么锁呢？

发现只加了一把锁，即IX意向排他锁（表锁，不影响插入）

在RR隔离级别下，如果select...for update的查询条件是普通索引的话，除了会加X锁，IX锁，还会加Gap 锁。

为什么不是一个锁表的X锁呢？ 这是因为:

> 若age列上没有索引，MySQL会走聚簇(主键)索引进行全表扫描过滤。每条记录都会加上X锁。但是，为了效率考虑，MySQL在这方面进行了改进，在扫描过程中，若记录不满足过滤条件，会进行解锁操作。同时优化违背了2PL原则。

Gap锁的提出，是为了解决幻读问题引入的，它是一种加在两个索引之间的锁。

也就是说RR隔离级别下，对于select...for update，查询条件无索引的话，会加一个IX锁（表锁，不影响插入），每一行实际记录行的X锁，还有对应于supremum pseudo-record的虚拟全表行锁。这种场景，通俗点讲，其实就是锁表了。

# 15.为什么不是唯一索引上加X锁就可以了呢？为什么主键索引上的记录也要加锁呢？

> 如果并发的一个SQL，通过唯一索引条件，来更新主键索引：update user_info_tab set user_name = '学友' where id = '1570068';此时，如果select...for update语句没有将主键索引上的记录加锁，那么并发的update就会感知不到select...for update语句的存在，违背了同一记录上的更新/删除需要串行执行的约束。

# 16.某个表有近千万数据，CRUD比较慢，如何优化？

说实话，这个数据量级， MySQL 单库单表支撑起来完全没有问题的，所以首先还是考虑数据库本身的优化。



![img](http://p3-sign.toutiaoimg.com/tos-cn-i-0022/440c9e3b6cd54bd88b42c67e89173449~tplv-tt-origin.jpeg?_iz=92253&from=wenda&x-expires=1681263969&x-signature=2hxYe65IEapB3KkaLPKH55txes8%3D)



从上图可以看到，数据库优化通常可以通过以上几点来实现：

- 硬件升级：也就是花更多的钱，升级我们数据库硬件配置，包括 CPU、内存、磁盘、网络等等，但是这个方案成本高，而且不一定能起到非常好的效果。
- 数据库配置：修改数据库的配置，有可能让我们的 CRUD 操作变得更快，不过我也不建议大家把经历放在这一点上面；首先，数据库的配置通常由专业的 DBA 来负责；第二，大部分时候，默认的数据库配置在大多数情况下已经是最优配置了。



对于开发人员来说，我们需要把注意力放在后面三点：



## 数据结构的优化，也就是表结构的优化

- 数据类型的选择：选用合适的数据结构。什么叫做"合适的数据结构"，比如性别字段，M表示男F表示女，那么一个 char(1) 就足够了，如果存储人的年龄，那么就没有必要使用 INT 这么大范围的字段了；
- 适当的拆分：千万不要试图把所有的字段放在一张表中，因为这会非常影响性能，通常一张表的字段最好不要超过 30 个；
- 适当的冗余：如果一些常用的字段，可能会用在不同的维度，那么我们可以把这些字段设计在多张表中，因为这样可能会减少表关联；
- 字段尽量设置成 not Null，尽量带有默认值。



## SQL 语句的优化

优化 SQL 语句执行速度的方法有很多，比如：

- 尽量使用索引，尽量避免全表扫描，提高查询速度；
- 当然你不能无限制地建立索引；维护索引也会影响性能，会降低 DML 操作的速度；
- 注意 SQL 语句的书写，有一些错误的写法可能会导致索引失效；
- 尽量避免在 where 子句中对字段进行 Null 值判断（当然我们在表设计中，直接建议不要有 Null）；
- 条件值多的情况下，尽量不要使用 in 和 not in ；
- select 的时候，使用具体的字段代替 * 号
- 避免返回大量数据，增加分页；



## 减少数据库的访问

- 我们可以通过增加本地缓存或分布式缓存的方式，将热点数据存储到缓存中，以减少数据库的访问；
- 终极大招，如果是一个不合理的需求，我们可以拒绝做这个需求，这样也算是"减少了数据库访问"。

![img](http://p3-sign.toutiaoimg.com/tos-cn-i-0022/bbcbe056e1d0404a9e2704f5a2981e0c~tplv-tt-origin.jpeg?_iz=92253&from=wenda&x-expires=1681263969&x-signature=BaQYPM%2BBhYY89i%2BLffE4fOV4Szk%3D)



说完了 MySQL 本身的优化，如果数据量进一步增大的话，我们还有什么优化的方案呢？

## 分库分表

分库分表包括分库和分表两个部分，在生产中通常包括：垂直分表、垂直分库、水平分表、水平分库四种方式。

## 垂直分表

把主键和一些列放在一个表，然后把主键和另外的列放在另一个表中。


垂直分表的优点：将一个大表拆分为不同包含不同模块的小表，可以让分别查询两个表的用户不受影响。
垂直分表的缺点：查询所有数据需要join操作，单标的数据量可能还是比较大。

## 垂直分库

垂直分库是指按照业务将表进行分类，分布到不同的数据库上面，每个库可以放在不同的服务器上，它的核心理念是专库专用。


垂直分库的优点：解决业务层面的耦合，业务清晰。
垂直分库的缺点：依然没有解决单表数据量过大的问题。

## 水平分表

保持数据表结构不变，通过某种策略将存储数据分片，这样每一片数据分散到不同的表中。 水平拆分可以支撑非常大的数据量。


水平分表的优点：优化单一表数据量过大而产生的性能问题。
水平分表的缺点：给应用增加复杂度，通常查询时需要多个表名，查询所有数据都需UNION操作。

## 水平分库

水平分库是把同一个表的数据按一定规则拆到不同的数据库中，每个库可以放在不同的服务器上。


水平分库的优点：解决了单库大数据，高并发的性能瓶颈。
水平分库的缺点：同一个表被分配在不同的数据库，查询时需要多个数据库。

## 主从复制+读写分离

### 主从复制

将主数据库中的DDL和DML操作通过二进制日志（BINLOG）传输到从数据库上，然后将这些日志重新执行（重做）；从而使得从数据库的数据与主数据库保持一致。

### 主从复制的过程

主服务器上面启动一个binlog线程，主服务器的任何修改都会保存在Binary log（二进制日志）里面。
从服务器上面启动一个I/O线程，连接到主服务器上面请求读取二进制日志，然后把读取到的二进制日志写到本地的一个Realy log（中继日志）里面。
从服务器上面开启一个SQL线程，定时检查Realy log，如果发现有更改立即把更改的内容在本机上面执行一遍。

### 主从复制的模式

异步复制模式：主库接受到客户请求后，处理完事务后，立即将结果返给客户端，不关心从库是否已经接收并处理。客户体验度高，访问从库可能没有数据。
全同步复制：当主库执行完一次事务，且所有从库都接受并处理之后，才返回给客户端。客户体验度差，数据同步好。
半同步复制：介于两种模式之间，主库执行一次事务后，等待至少一个从库接受并写到relay log中才返回给客户端。

### 主从复制的优点

做数据的备份，主数据库服务器故障后，可切换到从数据库继续工作，避免数据丢失。
可以做读写分离，使数据库能支持更大的并发。如果进程A使用master，进程B使用slave，那么进程B将不会影响进程A的速度。

### 主从复制的缺点

主从间的数据库不是实时同步，就算网络连接正常，也存在瞬间主从数据不一致的情况。
主服务器宕机，但是数据还没写入到binlog，可能会造成主从数据不一致。
读写分离
读写分离是依赖于主从复制，而主从复制又是为读写分离服务的。因为主从复制要求slave不能写只能读。

通过设置主从数据库实现读写分离，主数据库负责“写操作”，从数据库负责“读操作”，根据压力情况，从数据库可以部署多个提高“读”的速度，借此来提高系统总体的性能。

### 主从同步过程中，SQL线程可不可以并行执行？

不可以，因为这样可能SQL执行的顺序不同，导致事务提交时间有差异。

## 缓存redis

Redis是一个使用 C 语言开发的数据库，不过与传统数据库不同的是Redis 的数据是存在内存中的，也就是它是内存数据库，所以读写速度非常快，因此Redis被广泛应用于分布式缓存方向。

# 17.如何写sql能够有效的使用到复合索引

复合索引，也叫组合索引，用户可以在多个列上建立索引,这种索引叫做复合索引。

当我们创建一个组合索引的时候，如(k1,k2,k3)，相当于创建了（k1）、(k1,k2)和(k1,k2,k3)三个索引，这就是最左匹配原则。

select * from table where k1=A AND k2=B AND k3=D

有关于复合索引，我们需要关注查询Sql条件的顺序，确保最左匹配原则有效，同时可以删除不必要的冗余索引。

# 18.mysql中in 和exists的区别

区分 `in` 和 `exists` 主要是造成了驱动顺序的改变（这是性能变化的关键），如果是 `exists`，那么以外层表为驱动表，先被访问，如果是 `in` ，那么先执行子查询，所以我们会以驱动表的快速返回为目标，那么就会考虑到索引及结果集的关系 ，另外 `in` 是不对 `NULL` 进行处理。

当A表的数据集大于B表的数据集时，用in 优于 exists；
当A表的数据集小于B表的数据集， 用 exists 优于 in【注意： A与B表的id 字段应该建立索引】；
如果查询的两个表大小相当，那么用in和exists差别不大。如果两个表中一个较小，一个是大表，则子查询表大的用exists，子查询表小的用in；

# 19.使用自增id可能产生的问题

使用自增主键对数据库做分库分表，可能出现一些诸如主键重复等的问题，或者在数据库导入的时候，可能会因为主键出现一些问题。

- 最致命的一个缺点就是，很容易被别人知晓业务量，然后很容易被网络爬虫教做人
- 高并发的情况下，竞争自增锁会降低数据库的吞吐能力
- 数据迁移的时候，特别是发生表格合并这种操作的时候，会非常蛋疼

# 20.在MySQL中MVCC熟悉吗，它的底层原理？

您好， MVCC,多版本并发控制,它是通过读取历史版本的数据，来降低并发事务冲突，从而提高并发性能的一种机制。

MVCC需要关注这几个知识点：

```js
事务版本号
表的隐藏列
undo log
read view
```

什么是 MVCC？
全称Multi-Version Concurrency Control，即多版本并发控制，主要是为了提高数据库的并发性能。

每当数据在进行修改操作时，MySQL 会将所有的修改操作串联，形成一种数据的版本链条。根据事务隔离级别的不同，能够访问到的最新数据也不同，以此进行控制。

原理

重要字段
trx_id： 当前事务的 ID
roll_ptr： 回滚指针，指向下一个最新的 undo 日志版本
版本的组成
当前最新数据上存储有 roll_ptr 字段，指向下一个最新的 undo 日志版本
undo 日志上各条数据的回滚指针也会依次指向下一个版本的数据，以此形成一个版本链条

什么是 ReadView？
每一个事务在启动时，都会生成一个 ReadView，用来记录一些内容

ReadView 存储的内容：

m_ids： 当前系统中活跃的事务（说明事务未提交）
min_trx_id： 当前活跃事务中最小的那个
max_trx_id： 下一个即将生成的事务 ID（并非当前活跃事务中最大的）
creat_trx_id：生成 RV 的事务ID（这样就指定当前的 RV 属于哪个事务）
假如我再次修改，如何增加版本？
假如再次修改，修改的数据就会替换当前最新数据

而之前的数据就会下沉到 undo 日志的头部，可以理解为头插法

判断当前数据是否可见的 4 大准则
MySQL 根据四大准则，顺着版本链向下，找到第一个满足条件的数据，就是当前事务能看到的数据

如果 ReadView 中的 creator_trx_id 值 = trx_id 属性值 ，数据可见
理解： trx_id 属性值与 ReadView 中的 creator_trx_id 值相同，说明该版本是当前事务中的操作数据
如果ReadView 中的 min_trx_id 值 > trx_id 属性值 ，数据可见
理解：min_trx_id 比 trx_id 大，首先说明 trx_id 肯定不在存活事务范围内，其次说明 trx_id 事务是早发生的，并且结束，已经 commit 的事务
如果ReadView 中的 max_trx_id < trx_id 属性值，数据不可见
理解：最大的都比 trx_id 小，说明肯定是当前事务之后才启动的事务，因此肯定不可见
如果 min_trx_id <= trx_id <= max_trx_id。如果 trx_id 在存活事务范围内，数据不可见；如果不属于存活的事务，数据可见
理解：存活说明事务没结束因此不可见，不存活说明事务已经结束，因此可见。
PS：ReadView 中的参数才是当前事务的指标，而版本链上的 trx_id 是历史版本，别弄反了

# 21.数据库中间件了解过吗，sharding jdbc，mycat？

## 1、ShardingSphere介绍

ShardingSphere是一套开源的分布式数据库中间件解决方案，目前由Sharding-JDBC和Sharding-Proxy两款独立的产品组成，2020年4⽉16⽇正式成为 Apache 软件基⾦会的顶级项⽬。

- **Sharding-JDBC**：定位为轻量级**[\**java\**](https://www.bmabk.com/)**框架，在[Java](https://www.bmabk.com/index.php/post/tag/22)的JDBC层提供的额外服务，支持任意实现JDBC规范的数据库。
- **Sharding-Proxy**：定位为透明化的数据库代理端，通过实现数据库二进制协议，对异构语言提供支持。目前提供[**MySQL**](https://www.bmabk.com/index.php/post/tag/24)和PostgreSQL协议。

ShardingSphere利用**分布式场景下关系型数据库的计算和存储能力，提供标准化的数据分片、分布式事务和数据库治理功能**，可以将任意数据库转换为分布式数据库，适用于如Java同构、异构语言、容器、云原生等各种多样化的应用场景。

### 1.1 设计理念

ShardingSphere采用**Database Plus的设计理念**，致力于构建数据库上层的标准和生态，利用连接、增强和可插拔的方式在生态中补充数据库所缺失的能力。

> **Database Plus**：一种分布式数据库系统的设计理念。旨在碎片化的异构数据库上层构建生态，在最大限度的复用数据库原生存算能力的前提下，进一步提供面向全局的扩展和叠加计算能力（例如：数据分片、数据加密等）。使应用和数据库间的交互面向Database Plus构建的标准，从而屏蔽数据库碎片化对上层业务带来的差异化影响。

![分布式数据库中间件Sharding-JDBC介绍](https://www.bmabk.com/wp-content/uploads/2022/12/4-1672048652.png)

来源：ShardingSphere官网

**1）连接：打造数据库上层标准**

通过对数据库协议、SQL方言以及数据库存储的灵活适配，快速构建多模异构数据库上层的标准，同时通过内置DistSQL为应用提供标准化的连接方式。

**2）增强：数据库计算增强引擎**

在原生数据库基础能力之上，提供分布式及流量增强方面的能力。前者可突破底层数据库在计算与存储上的瓶颈，后者通过对流量的变形、重定向、治理、鉴权及分析能力提供更为丰富的数据应用增强能力。

**3）可插拔：构建数据库功能生态**

ShardingSphere的可插拔架构划分为3层，它们是：L1内核层、L2功能层、L3生态层

- **L1内核层**：是数据库基本能力的抽象，其所有组件均必须存在，但具体实现方式可通过可插拔的方式更换。主要包括查询优化器、分布式事务引擎、分布式执行引擎、权限引擎和调度引擎等。
- **L2功能层**：用于提供增量能力，其所有组件均是可选的，可以包含零至多个组件。组件之间完全隔离，互无感知，多组件可通过叠加的方式相互配合使用。主要包括数据分片、读写分离、数据库高可用、数据加密、影子库等。 用户自定义功能可完全面向Apache ShardingSphere定义的顶层接口进行定制化扩展，而无需改动内核代码。
- **L3生态层**：用于对接和融入现有数据库生态，包括数据库协议、SQL解析器和存储适配器，分别对应于Apache ShardingSphere以数据库协议提供服务的方式、SQL方言操作数据的方式以及对接存储节点的数据库类型。

### 1.2 产品规划

随着版本的不断迭代，ShardingSphere的功能也变得多元化起来：从最开始Sharding-JDBC 1.0版本只有数据分片，到2.0版本开始支持数据库治理，再到3.0版本Sharding-Proxy上线并支持分布式事务，再到4.0进入Apache基金会，再到如今5.0版本的可插拔式设计。其宗旨是**构建数据库上层的标准和生态，建立统一标准和规范，完善数据库能力。**

![分布式数据库中间件Sharding-JDBC介绍](https://www.bmabk.com/wp-content/uploads/2022/12/6-1672048653.png)

来源：ShardingSphere官网

### 1.3 Sharding-JDBC

接下来重点介绍Sharding-JDBC框架，Sharding-JDBC定位为轻量级Java框架，在Java的JDBC层提供的额外服务。它使用客户端直连数据库，以jar包形式提供服务，无需额外部署和依赖，可理解为增强版的JDBC驱动，完全兼容JDBC和各种ORM框架。

- 适用于任何基于JDBC的ORM框架，如：JPA, Hibernate, [**Mybatis**](https://www.bmabk.com/index.php/post/tag/34), **[\**spring\**](https://www.bmabk.com/)** JDBC Template或直接使用JDBC；
- 支持任何第三方的数据库连接池，如：DBCP, C3P0, BoneCP, HikariCP等；
- 支持任意实现JDBC规范的数据库，目前支持[MySQL](https://www.bmabk.com/index.php/post/tag/24)，PostgreSQL，Oracle，SQLServer以及任何可使用JDBC访问的数据库。

![分布式数据库中间件Sharding-JDBC介绍](https://www.bmabk.com/wp-content/uploads/2022/12/8-1672048653.png)

#### 1.3.1 Sharding-JDBC主要功能

Sharding-JDBC主要支持**数据分片、分布式事务和数据库治理**功能。

![分布式数据库中间件Sharding-JDBC介绍](https://www.bmabk.com/wp-content/uploads/2022/12/5-1672048653.png)

1. **数据分片**

2. 1. 分库分表：将数据分片进行垂直拆分和水平拆分
   2. 读写分离：根据SQL语义的分析，将读操作和写操作分别路由至主库与从库
   3. 分片策略：基于分片键和不同的分片算法实现分片
   4. 分布式主键：内置的分布式主键生成器，例如UUID、SNOWFLAKE

3. **分布式事务**

4. 1. 标准化的事务接口：begin/commit/rollback接口实现
   2. XA强一致性事务：AP、TM和RM模型保证分布式事务一致性
   3. 柔性事务：BASE事务模型实现事务的最终一致性

5. **数据库治理**

6. 1. 数据库网关：SQL方言自动翻译功能，将不同类型的数据库方言自动翻译为后端数据库所使用的方言
   2. 流量治理：计算节点的过载保护和数据节点限流
   3. 数据加密：数据加密功能，实现数据的合规化改造治理
   4. **可视化链路跟踪**：多种监控性能和监控指标，实现监控仪表盘和应用链路跟踪

#### 1.3.2 Sharding-JDBC处理流程

Sharding-JDBC定位为Java框架，对于开发人员只需要使用调用JDBC API访问数据库，只要正确使用DataSource、Connection、Statement 、ResultSet 等API接口，直接操作数据库即可，实现“*创建DataSource->获取Connection->构建Statement->执行SQL语句->处理ResultSet*”完整的流程。Sharding-JDBC就是将原来的DataSource、Connection等接口扩展成 ShardingDataSource、ShardingConnection，对外暴露的JDBC操作接口与JDBC规范中的接口完全一致。因此，Sharding-JDBC适用于任何基于JDBC的ORM框架，并完美兼容任何第三方的数据库连接池。

![分布式数据库中间件Sharding-JDBC介绍](https://www.bmabk.com/wp-content/uploads/2022/12/1-1672048654.png)

## 2、Sharding-JDBC核心原理

### 2.1 分片概念

数据分片通常将原本一张数据量很大的表根据分片规则和分片键拆分为表结构完全一样的小数据量的表，如下图所示，每张表只存储大表中的一部分数据。当SQL执行时会通过不同的分片策略，访问不同的库和分片中的数据。

![分布式数据库中间件Sharding-JDBC介绍](https://www.bmabk.com/wp-content/uploads/2022/12/7-1672048654.png)

#### 2.1.1 表概念

1. **逻辑表**：相同结构的水平拆分数据库（表）的逻辑名称，是SQL中表的逻辑标识。比如图中的t_prod，拆分后数据库中已经不存在这张表，取而代之的是t_prod_n这些表，t_prod就称为这些拆分表的逻辑表。
2. **真实表**：数据库中真实存在的物理表，比如图中的t_prod_0…t_prod_3
3. **数据节点**：分库分表后不可再分割的最小的数据单元，由数据源名称和数据表组成，比如t_prod_db_1.t_prod_0就表示一个数据节点
4. **绑定表**：指代分片规则一致的一组分片表，比如t_prod和t_order均按照分片键id进行分片，则t_prod和t_order为绑定表。使用绑定表进行多表关联查询时，必须使用分片键进行关联，否则会出现笛卡尔积关联或跨库关联，从而影响查询效率。
5. **广播表**：指所有的分片数据源中都存在的表，表结构及其数据在每个数据库中均完全一致。适用于数据量不大且需要与海量数据的表进行关联查询的场景，如字典表、参数表。
6. **单表**：指所有的分片数据源中仅唯一存在的表。适用于数据量不大且无需分片的表。

#### 2.1.2 分片算法

Sharding-JDBC对分片键基于一定的分片规则实现数据分片，从执行SQL的角度来看分片算法理解为一种路由机制。Sharding-JDBC支持以下内置的分片算法：

1. **精确分片算法**（PreciseShardingAlgorithm）：用于单个字段作为分片键，SQL中有“=”和“IN”等条件的分片，需要在标准分片策略（StandardShardingStrategy）下使用。
2. **范围分片算法**（RangeShardingAlgorithm）：范围分片算法用于单个字段作为分片键，SQL 中有“BETWEEN AND、>、<、>=、<=”下使用。
3. **复合分片算法**（ComplexKeysShardingAlgorithm）：用于多个字段作为分片键的分片操作，同时获取到多个分片健的值，根据多个字段处理业务逻辑。需要在复合分片策略（ComplexShardingStrategy）下使用。
4. **Hint分片算法**（HintShardingAlgorithm）：上边的算法中我们都是解析语句提取分片键，并设置分片策略进行分片。但有些时候我们并没有使用任何的分片键和分片策略，可还想将SQL路由到目标数据库和表，就需要通过手动干预指定SQL的目标数据库和表信息，这也叫强制路由。

#### 2.1.3 分片策略

分片策略是一个抽象的概念，实际的分片操作是**由分片键+分片算法**实现的。

1. **标准分片策略**：适用于单分片键，此策略支持PreciseShardingAlgorithm和RangeShardingAlgorithm 两个分片算法。
2. **复合分片策略**：支持多分片键，同样支持对SQL语句中的“=、>、<、>=、<=、IN、BETWEEN AND”的分片操作。
3. **行表达式分片策略**：支持对 SQL语句中的“=、IN”的分片操作，但只支持单分片键。这种策略通常用于简单的分片，不需要自定义分片算法，可以直接在配置文件中写规则。
4. **Hint分片策略**：对应Hint分片算法，通过指定分片健而非从 SQL中提取分片健的方式进行分片的策略。

### 2.2 数据分片原理

Sharding-JDBC数据分片的原理如图所示，分为“**SQL解析->执⾏器优化->SQL路由->SQL改写->SQL执⾏->结果归并**”过程。

![分布式数据库中间件Sharding-JDBC介绍](https://www.bmabk.com/wp-content/uploads/2022/12/0-1672048654.png)

#### 2.2.1 SQL解析

SQL解析分为**词法解析和语法解析**。先通过词法解析器将SQL拆分为一个个不可再分的单词，再使用语法解析器对SQL进行理解，并最终提炼出解析上下文。解析上下文包括表、选择项、排序项、分组项、聚合函数、分页信息、查询条件以及可能需要修改的占位符的标记。

```
SELECT id, name FROM t_user WHERE status = 'ACTIVE' AND age > 18
```

以上SQL语句，解析之后抽象的语法树如下：

![分布式数据库中间件Sharding-JDBC介绍](https://www.bmabk.com/wp-content/uploads/2022/12/2-1672048655.png)

通过对抽象语法树遍历，提炼出分片所需的上下文，上下文包含查询字段信息（Field）、表信息（Table）、查询条件（Condition）、排序信息（Order By）、分组信息（Group By）以及分页信息（Limit）等，并标记出 SQL中有可能需要改写的位置。

#### 2.2.2 执行器优化

由Federation执行引擎（开发中）提供支持，**对关联查询、子查询等复杂查询进行优**化，同时支持跨多个数据库实例的分布式查询，内部使用关系代数优化查询计划，通过最优计划查询出结果。

#### 2.2.3 SQL路由

SQL路由**根据解析上下文匹配数据库和表的分片策略，并生成路由路径**。对于携带分片键的SQL，根据分片键的不同可以划分为单片路由（分片键的操作符是等号）、多片路由（分片键的操作符是IN）和范围路由（分片键的操作符是BETWEEN）。简单点理解就是可以根据配置的分片策略计算出SQ该在哪个库的哪个表中执行，而SQL路由又根据有无分片健区分出分片路由和广播路由。

![分布式数据库中间件Sharding-JDBC介绍](https://www.bmabk.com/wp-content/uploads/2022/12/2-1672048655-1.png)

1）**分片路由：**用于根据分片键进行路由的场景，又细分为直接路由、标准路由和笛卡尔积路由这3种类型。

- **直接路由：**满足直接路由的条件相对苛刻，它需要通过Hint（使用HintAPI直接指定路由至库表）方式分片，并且是只分库不分表的前提下，则可以避免SQL解析和之后的结果归并。
- **标准路由：**当分片运算符是等于号时，路由结果将落入单库（表）；当分片运算符是BETWEEN或IN时，则路由结果不一定落入唯一的库（表），因此一条逻辑SQL最终可能被拆分为多条用于执行的真实SQL。
- **笛卡尔积路由：**它无法根据绑定表的关系定位分片规则，因此非绑定表之间的关联查询需要拆解为笛卡尔积组合执行。

2）**广播路由：**对于不携带分片键的SQL，则采取广播路由的方式。根据SQL类型又可以划分为全库表路由、全库路由、全实例路由、单播路由和阻断路由这5种类型。

- **全库表路由：**全库表路由用于处理对数据库中与其逻辑表相关的所有真实表的操作，主要包括不带分片键的DQL和DML以及DDL等
- **全库路由：**全库路由用于处理对数据库的操作，包括用于库设置的SET类型的数据库管理命令，以及TCL这样的事务控制语句。
- **全实例路由：**全实例路由用于DCL操作，授权语句针对的是数据库的实例。
- **单播路由：**单播路由用于获取某一真实表信息的场景，它仅需要从任意库中的任意真实表中获取数据即可。
- **阻断路由：**阻断路由用于屏蔽 SQL 对数据库的操作，比如use database

#### 2.2.4 SQL改写

SQL改写是将SQL改写为在真实数据库中可以正确执行的语句。SQL改写分为**正确性改写和优化改写。**

- **正确性改写：**在包含分表的场景中，需要将分表配置中的逻辑表名称改写为路由之后所获取的真实表名称。另外还包括包括补列和分页信息修正等内容。
- **优化改写：**在不影响查询正确性的情况下，对性能进行提升的有效手段。分为单节点优化和流式归并优化

![分布式数据库中间件Sharding-JDBC介绍](https://www.bmabk.com/wp-content/uploads/2022/12/5-1672048656.png)

#### 2.2.5 SQL执行

SQL执行引擎采用一套自动化的执行引擎，负责将路由和改写完成之后的真实SQL安全且高效发送到底层数据源执行。

![分布式数据库中间件Sharding-JDBC介绍](https://www.bmabk.com/wp-content/uploads/2022/12/3-1672048656.png)

SQL执行引擎分为**准备阶段和执行阶段**：

1）准备阶段

准备阶段用于准备执行的数据，分为**结果集分组和执行单元创建**两个步骤。

- 结果集分组：将SQL的路由结果按照数据源的名称进行分组
- 通过上一步骤获得的路由分组结果创建执行的单元

2）执行阶段

执行阶段用于真正的执行SQL，它分为**分组执行和归并结果集生成**两个步骤

- 分组执行将准备执行阶段生成的执行单元分组下发至底层并发执行引擎，并针对执行过程中的每个关键步骤发送事件。
- 归并结果集生成通过在执行准备阶段的获取的连接模式，生成内存归并结果集或流式归并结果集，并将其传递至结果归并引擎，以进行下一步的工作

#### 2.2.6 SQL结果归并

SQL结果归并是**将多个执行结果集归并以便于通过统一的JDBC接口输出**。结果归并从功能上分为遍历、排序、分组、分页和聚合5种类型；从结构划分，可分为流式归并、内存归并和装饰者归并。

- **遍历归并**：将多个数据结果集合并为一个单向链表，在遍历完成链表中当前数据结果集之后，将链表元素后移一位，继续遍历下一个数据结果集即可
- **排序归并**：每个数据结果集自身是有序的，因此只需要将数据结果集当前游标指向的数据值进行排序即可，不需要对所有的结果集进行全量的排序，节省了内存的消耗
- **分组归并**：分为流式分组归并和内存分组归并， 流式分组归并要求SQL的排序项与分组项的字段以及排序类型（ASC或DESC）必须保持一致。如果不一致，需要将所有的结果集数据加载至内存中进行分组和聚合。
- **聚合归并**：使用聚合函数进行比较、累加和求平均值
- **分页归并**：通过装饰者模式来增加对数据结果集进行分页的能力，将无需获取的数据过滤掉。

![分布式数据库中间件Sharding-JDBC介绍](https://www.bmabk.com/wp-content/uploads/2022/12/10-1672048656.png)

## 3、Sharding-JDBC和Mycat对比

Mycat和ShardingSphere（Sharding-JDBC）都是非常流行的开源分布式数据库中间件，各自具有一些独特的功能，也有很多企业成功应用的案例。二者之间的特性对比如下表所示：

| 特性           | Mycat             | Sharding-JDBC |
| -------------- | ----------------- | ------------- |
| 开发语言       | Java              | Java          |
| 开源协议       | GPL-2.0/GPL-3.0   | Apache-2.0    |
| 数据库         | 多种              | 多种          |
| 连接数         | 低                | 高            |
| 应用语言       | 任意              | Java          |
| 代码入侵       | 无                | 需修改代码    |
| 性能           | 损耗略高          | 损耗低        |
| 无中心化       | 无                | 是            |
| 管理控制台     | Mycat-web         | ShardingUI    |
| 分库分表       | 单库多表/多库单表 | 支持          |
| 多租户方案     | 支持              | 无            |
| 读写分离       | 支持              | 支持          |
| 分片策略定制化 | 支持              | 支持          |
| 分布式主键     | 支持              | 支持          |
| 标准化事务接口 | 支持              | 支持          |
| XA强一致性事务 | 支持              | 支持          |
| 柔性事务       | 无                | 支持          |
| 配置动态化     | 开发中            | 支持          |
| 编排治理       | 开发中            | 支持          |
| 数据脱敏       | 无                | 支持          |
| 可视化链路跟踪 | 无                | 支持          |
| 多节点操作     | 支持              | 支持          |
| 跨库关联       | 跨库2表JOIN       | 无            |
| IP白名单       | 支持              | 无            |
| SQL黑名单      | 支持              | 无            |
| 存储过程       | 支持              | 无            |

Mycat是一个分布式数据库中间件，对[**前端**](https://www.bmabk.com/index.php/post/tag/32)来说相当于一个数据库代理。ShardingSphere定位为关系型数据库中间件，旨在充分合理地在分布式的场景下利用关系型数据库的计算和存储能力，而并非实现一个全新的关系型数据库。目前来看接入Apache基金会的ShardingSphere体系更加完善，社区更加活跃。

# 22.MYSQL的主从延迟，你怎么解决？

## 什么是主从延迟

在讨论如何[解决](https://worktile.com/kb/tag/解决)主从延迟之前，我们先了解下什么是主从延迟。

为了完成主从复制，从库需要通过 I/O 线程获取主库中 dump 线程读取的 binlog 内容并写入到自己的中继日志 relay log 中，从库的 SQL 线程再读取中继日志，重做中继日志中的日志，相当于再执行一遍 SQL，更新自己的数据库，以达到**数据的一致性**。

与数据同步有关的时间点主要包括以下三个：

1. 主库执行完一个事务，写入 binlog，将这个时刻记为 T1；
2. 之后传给从库，将从库接收完这个 binlog 的时刻记为 T2；
3. 从库执行完成这个事务，将这个时刻记为 T3。

所谓主从延迟，就是同一个事务，从库执行完成的时间与主库执行完成的时间之差，也就是 `T3 - T1`。

可以在备库上执行 `show slave status` 命令，它的返回结果里面会显示 `seconds_behind_master`，用于表示当前备库延迟了多少秒。
`seconds_behind_master` 的计算方法是这样的：

1. 每个事务的 binlog 里面都有一个时间字段，用于记录主库上写入的时间；
2. 备库取出当前正在执行的事务的时间字段的值，计算它与当前系统时间的差值，得到 `seconds_behind_master`。

在网络正常的时候，日志从主库传给从库所需的时间是很短的，即 `T2 - T1` 的值是非常小的。也就是说，网络正常情况下，主从延迟的主要来源是从库接收完 binlog 和执行完这个事务之间的时间差。

由于主从延迟的存在，我们可能会发现，数据刚写入主库，结果却查不到，因为可能还未同步到从库。主从延迟越严重，该问题也愈加明显。

## 主从延迟的来源

主库和从库在执行同一个事务的时候出现时间差的问题，主要原因包括但不限于以下几种情况：

- 有些部署条件下，**从库所在机器的性能要比主库性能差**。
- **从库的压力较大**，即从库承受了大量的请求。
- **执行大事务**。因为主库上必须等事务执行完成才会写入 binlog，再传给备库。如果一个主库上语句执行 10 分钟，那么这个事务可能会导致从库延迟 10 分钟。
- **从库的并行复制能力**。

## 主从延迟的解决方案



1. 降低多线程大事务并发的概率，优化业务逻辑

2. 优化SQL，避免慢SQL， 减少批量操作 ，建议写脚本以update-sleep这样的形式完成。

3. 提高从库机器的配置 ，减少主库写binlog和从库读binlog的效率差。

4. 尽量采用 短的链路 ，也就是主库和从库服务器的距离尽量要短，提升端口带宽，减少binlog传输

的网络延时。

5. 实时性要求的业务读强制走主库，从库只做灾备，备份

解决主从延迟主要有以下方案：

### 1.**配合 semi-sync 半同步复制**；

### **2.一主多从**，分摊从库压力；

### **3.强制走主库方案**（强一致性）；

### 4.sleep 方案：主库更新后，读从库之前先 sleep 一下；

判断主备无延迟方案（例如判断 `seconds_behind_master` 参数是否已经等于 0、对比位点）；

### **5.并行复制** — 解决从库复制延迟的问题；

这里主要介绍我在项目中使用的几种方案，分别是**半同步复制、实时性操作强制走主库、并行复制**。

#### **semi-sync 半同步复制**

MySQL 有三种同步模式，分别是：

**「异步复制」**：MySQL 默认的复制即是异步的，主库在执行完客户端提交的事务后会立即将结果返给客户端，并不关心从库是否已经接收并处理。这样就会有一个问题，一旦主库宕机，此时主库上已经提交的事务可能因为网络原因并没有传到从库上，如果此时执行故障转移，强行将从提升为主，可能导致新主上的数据不完整。

**「全同步复制」**：指当主库执行完一个事务，并且所有的从库都执行了该事务，主库才提交事务并返回结果给客户端。因为需要等待所有从库执行完该事务才能返回，所以全同步复制的性能必然会收到严重的影响。

**「半同步复制」**：是介于全同步复制与全异步复制之间的一种，主库只需要等待至少一个从库接收到并写到 Relay Log 文件即可，主库不需要等待所有从库给主库返回 ACK。主库收到这个 ACK 以后，才能给客户端返回 “事务完成” 的确认。

**MySQL 默认的复制是异步的，所以主库和从库的数据会有一定的延迟，更重要的是异步复制可能会引起数据的丢失**。但是全同步复制又会使得完成一个事务的时间被拉长，带来性能的降低。因此我把目光转向半同步复制。**从 MySQL 5.5 开始，MySQL 以插件的形式支持 semi-sync 半同步复制**。

相对于异步复制，半同步复制提高了数据的安全性，减少了主从延迟，当然它也还是有一定程度的延迟，这个延迟最少是一个 TCP/IP 往返的时间。所以，**半同步复制最好在低延时的网络中使用**。

> 需要注意的是：
>
> - 主库和从库都要启用半同步复制才会进行半同步复制功能，否则主库会还原为默认的异步复制。
> - 如果在等待过程中，等待时间已经超过了配置的超时时间，没有收到任何一个从库的 ACK，那么此时主库会自动转换为异步复制。当至少一个半同步从节点赶上来时，主库便会自动转换为半同步复制。

##### 半同步复制的潜在问题

在传统的半同步复制中（MySQL 5.5 引入），主库写数据到 binlog，并且执行 commit 提交事务后，会一直等待一个从库的 ACK，即从库写入 Relay Log 后，并将数据落盘，再返回给主库 ACK，主库收到这个 ACK 以后，才能给客户端返回 “事务完成” 的确认。

![MySQL主从延迟问题怎么解决](https://worktile.com/kb/wp-content/uploads/2022/09/44599.jpg)

这样会出现一个问题，就是实际上主库已经将该事务 commit 到了存储引擎层，应用已经可以看到数据发生了变化，只是在等待返回而已。如果此时**主库宕机**，可能从库还没写入 Relay Log，就会发生**主从库数据不一致**。

为了解决上述问题，**MySQL 5.7 引入了增强半同步复制**。针对上面这个图，“Waiting Slave dump” 被调整到了 “Storage Commit” 之前，即主库写数据到 binlog 后，就开始等待从库的应答 ACK，直到至少一个从库写入 Relay Log 后，并将数据落盘，然后返回给主库 ACK，通知主库可以执行 commit 操作，然后**主库再将事务提交到事务引擎层**，应用此时才可以看到数据发生了变化。

![MySQL主从延迟问题怎么解决](https://worktile.com/kb/wp-content/uploads/2022/09/44600.jpg)

> 当然之前的半同步方案同样支持，MySQL 5.7.2 引入了一个新的参数 `rpl_semi_sync_master_wait_point` 进行控制。这个参数有两种取值：
>
> 1. AFTER_SYNC：这个是新的半同步方案，Waiting Slave dump 在 Storage Commit 之前。
> 2. AFTER_COMMIT：这个是老的半同步方案。

在 MySQL 5.5 – 5.6 使用 after_commit 的模式下，客户端事务在存储引擎层提交后，在主库等待从库确认的过程中，主库宕机了。此时，结果虽然没有返回给当前客户端，但事务已经提交了，其他客户端会读取到该已提交的事务。如果从库没有接收到该事务或者未写入 relay log，同时主库宕机了，之后切换到备库，那么之前读到的事务就不见了，出现了幻读，也就是数据丢失了。

MySQL 5.7 默认值则是 after_sync，主库将每个事务写入 binlog，传给从库并刷新到磁盘 (relay log)。主库等到从库返回 ack 之后，再提交事务并且返回 commit OK 结果给客户端。 即使主库 crash，所有在主库上已经提交的事务都能保证已经同步到从库的 relay log 中，解决了 after_commit 模式带来的幻读和数据丢失问题，**故障切换时数据一致性将得到提升**。因为从库没有写入成功的话主库也不会提交事务。并且在 commit 之前等待从库 ACK，还可以堆积事务，有利于 group commit 组提交，有利于提升性能。

> 但这样也会有个问题，假设主库在存储引擎提交之前挂了，那么很明显这个事务是不成功的，但由于对应的 Binlog 已经做了 Sync 操作，从库已经收到了这些 Binlog，并且执行成功，相当于在从库上多了数据（从库上有该数据而主库没有），也算是有问题的，但多了数据一般不算严重的问题。它能保证的是不丢数据，多了数据总比丢数据要好。

#### 一主多从

如果从库承担了大量查询请求，那么从库上的查询操作将耗费大量的 CPU 资源，从而影响了同步速度，造成主从延迟。那么我们可以多接几个从库，让这些从库来共同分担读的压力。

简而言之，就是加机器，方法简单粗暴，但也会带来一定成本。

#### **强制走主库方案**

如果某些操作对数据的实时性要求比较苛刻，需要反映实时最新的数据，比如说涉及金钱的金融类系统、在线实时系统、又或者是写入之后马上又读的业务，这时我们就得放弃读写分离，让此类的读请求也走主库，这就不存延迟问题了。

当然这也失去了读写分离带给我们的性能提升，需要适当取舍。

#### **并行复制**

一般 MySQL 主从复制有三个线程参与，都是单线程：Binlog Dump 线程、IO 线程、SQL 线程。复制出现延迟一般出在两个地方：

- SQL 线程忙不过来（主要原因）；
- 网络抖动导致 IO 线程复制延迟（次要原因）。

日志在备库上的执行，就是备库上 SQL 线程执行中继日志（relay log）更新数据的逻辑。

在 MySQL 5.6 版本之前，MySQL 只支持单线程复制，由此在主库并发高、TPS 高时就会出现严重的主备延迟问题。**从 MySQL 5.6 开始有了多个 SQL 线程的概念，可以并发还原数据，即并行复制技术**。这可以很好的解决 MySQL 主从延迟问题。

从单线程复制到最新版本的多线程复制，中间的演化经历了好几个版本。其实说到底，所有的多线程复制机制，都是要把只有一个线程的 sql_thread，拆成多个线程，也就是都符合下面的这个多线程模型：

**coordinator 就是原来的 sql_thread，不过现在它不再直接更新数据了，只负责读取中转日志和分发事务。真正更新日志的，变成了 worker 线程**。而 worker 线程的个数，就是由参数 `slave_parallel_workers` 决定的。

由于 worker 线程是并发运行的，为了保证事务的隔离性以及不会出现更新覆盖问题，coordinator 在分发的时候，需要满足以下这两个基本要求：

1. **更新同一行的两个事务，必须被分发到同一个 worker 中（避免更新覆盖）**。
2. **同一个事务不能被拆开，必须放到同一个 worker 中（保证事务隔离性）**。

各个版本的多线程复制，都遵循了这两条基本原则。

以下是按表分发策略和按行分发策略，可以帮助理解 MySQL 官方版本并行复制策略的迭代：

- **按表分发策略**：如果两个事务更新不同的表，它们就可以并行。因为数据是存储在表里的，所以按表分发，可以保证两个 worker 不会更新同一行。

- - 按表分发的方案，在多个表负载均匀的场景里应用效果很好，但缺点是：如果碰到热点表，比如所有的更新事务都会涉及到某一个表的时候，所有事务都会被分配到同一个 worker 中，就变成单线程复制了。

- **按行分发策略**：如果两个事务没有更新相同的行，则它们在备库上可以并行。显然，这个模式要求 binlog 格式必须是 row。

- - 按行并行复制的方案解决了热点表的问题，并行度更高，但缺点是：相比于按表并行分发策略，按行并行策略在决定线程分发的时候，需要消耗更多的计算资源。

##### MySQL 5.6 版本的并行复制策略

MySQL 5.6 版本，支持了并行复制，只是支持的粒度是**按库并行（基于 Schema）**。

**其核心思想是**：不同 schema 下的表并发提交时的数据不会相互影响，即从库可以对 relay log 中不同的 schema各分配一个类似 SQL 线程功能的线程，来重放 relay log 中主库已经提交的事务，保持数据与主库一致。

如果在主库上有多个 DB，使用这个策略对于从库复制的速度可以有比较大的提升。但通常情况下都是单库多表，那基于库的并发也就没有什么作用，根本无法并行重放，所以这个策略用得并不多。

##### MySQL 5.7 的并行复制策略

**MySQL 5.7 引入了基于组提交的并行复制**，参数 `slave_parallel_workers` 设置并行线程数，由参数 `slave-parallel-type` 来控制并行复制策略：

- 配置为 DATABASE，表示使用 MySQL 5.6 版本的按库并行策略；
- 配置为 LOGICAL_CLOCK，表示使用基于组提交的并行复制策略；

利用 binlog 的组提交 (group commit) 机制，可以得出一个组提交的事务都是可以并行执行的，原因是：**能够在同一组里提交的事务，一定不会修改同一行（由于 MySQL 的锁机制），因为事务已经通过锁冲突的检验了**。

**基于组提交的并行复制具体流程如下**：

1. 在一组里面一起提交的事务，有一个相同的 commit_id，下一组就是 commit_id+1；commit_id 直接写到 binlog 里面；
2. 传到备库应用的时候，相同 commit_id 的事务分发到多个 worker 执行；
3. 这一组全部执行完成后，coordinator 再去取下一批执行。

**所有处于 prepare 和 commit 状态的事务都是可以在备库上并行执行的**。

binlog 的组提交的两个有关参数：

- binlog_group_commit_sync_delay 参数，表示延迟多少微秒后才调用 fsync 刷盘；
- binlog_group_commit_sync_no_delay_count 参数，表示累积多少次以后才调用 fsync。

这两个参数是用于故意拉长 binlog 从 write 到 fsync 的时间，以此减少 binlog 的写盘次数。在 MySQL 5.7 的并行复制策略里，它们可以用来制造更多的“同时处于 prepare 阶段的事务”。可以考虑调整这两个参数值，来达到提升备库复制并发度的目的。

#  23.什么是数据库连接池?为什么需要数据库连接池呢?

数据库连接是一种关键的、有限的、昂贵的资源，这一点在[多用户](https://baike.baidu.com/item/多用户/4381464?fromModule=lemma_inlink)的网页应用程序中体现得尤为突出。对数据库连接的管理能显著影响到整个应用程序的[伸缩性](https://baike.baidu.com/item/伸缩性/9083908?fromModule=lemma_inlink)和[健壮性](https://baike.baidu.com/item/健壮性/4430133?fromModule=lemma_inlink)，影响到程序的[性能指标](https://baike.baidu.com/item/性能指标/22062856?fromModule=lemma_inlink)。数据库连接池正是针对这个问题提出来的。

数据库连接池负责分配、管理和释放数据库连接，它允许[应用程序](https://baike.baidu.com/item/应用程序/5985445?fromModule=lemma_inlink)重复使用一个现有的数据库连接，而不是再重新建立一个；释放[空闲时间](https://baike.baidu.com/item/空闲时间/19029263?fromModule=lemma_inlink)超过最大空闲时间的数据库连接来避免因为没有释放数据库连接而引起的数据库连接遗漏。这项技术能明显提高对[数据库操作](https://baike.baidu.com/item/数据库操作/20864618?fromModule=lemma_inlink)的性能。

# 24.一条SQL语句在MySQL中如何执行的？

我们看到的只是输入一条语句，返回一个结果，却不知道这条语句在 MySQL 内部的执行过程。

MySQL 的基本架构示意图，从中你可以清楚地看到 SQL 语句在 MySQL 的各个功能模块中的执行过程。

[![img](https://img2020.cnblogs.com/blog/1365470/202003/1365470-20200330140529330-934488719.png)](https://img2020.cnblogs.com/blog/1365470/202003/1365470-20200330140529330-934488719.png)

（图源https://blog.csdn.net/Megustas_JJC/article/details/84380108）

大体来说，MySQL 可以分为 Server 层和存储引擎层两部分。

- Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。
- 而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。

也就是说，你执行 create table 建表的时候，如果不指定引擎类型，默认使用的就是 InnoDB。不过，你也可以通过指定存储引擎的类型来选择别的引擎，比如在 create table 语句中使用 engine=memory, 来指定使用内存引擎创建表。

### 查询缓存

由于表经常更新，查询缓存的失效频繁，查询缓存往往利大于弊。，MySQL 8.0 版本开始直接将查询缓存的整块功能删掉了。

### 优化器

经过了分析器，MySQL 就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。

优化器是**在表里面有多个索引的时候，决定使用哪个索引**；或者在一个语句**有多表关联（join）的时候，决定各个表的连接顺序**。比如你执行下面这样的语句，这个语句是执行两个表的 join：



```
mysql> select * from t1 join t2 using(ID)  where t1.c=10 and t2.d=20;
```

- 既可以先从表 t1 里面取出 c=10 的记录的 ID 值，再根据 ID 值关联到表 t2，再判断 t2 里面 d 的值是否等于 20。
- 也可以先从表 t2 里面取出 d=20 的记录的 ID 值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10。
  这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。

如果你还有一些疑问，比如优化器是怎么选择索引的，有没有可能选择错等等，没关系，我会在后面的文章中单独展开说明优化器的内容。

### 执行器

开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误，如下所示：



```
mysql> select * from T where ID=10;

ERROR 1142 (42000): SELECT command denied to user 'b'@'localhost' for table 'T'
```

如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去**使用这个引擎提供的接口**。

比如我们这个例子中的表 T 中，ID 字段没有索引，那么执行器的执行流程是这样的：

1. 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中；
2. 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。
3. 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。

至此，这个语句就执行完成了。

对于有索引的表，执行的逻辑也差不多。第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。

- 既可以先从表 t1 里面取出 c=10 的记录的 ID 值，再根据 ID 值关联到表 t2，再判断 t2 里面 d 的值是否等于 20。
- 也可以先从表 t2 里面取出 d=20 的记录的 ID 值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10。
  这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。

上面是一条查询sql，再看一条SQL语句的执行过程，更新操作：



```
update tb_student A set A.age='19' where A.name='张三'; 
```

其实条语句也基本上会沿着上一个查询的流程走，**只不过执行更新的时候肯定要记录日志啦**，这就会引入日志模块了，mysql 自带的日志模块式binlog（归档日志），所有的存储引擎都可以使用，我们常用的InnoDB引擎还自带了一个日志模块redo log，我们就以InnoDB模式下来探讨这个语句的执行流程。流程如下：

-  先查询到张三这一条数据，如果有缓存，也是会用到缓存。
-  然后拿到查询的语句，把 age 改为19，然后调用引擎API接口，写入这一行数据，InnoDB引擎把数据保存在内存中，同时记录redo log，此时redo log进入prepare状态，然后告诉执行器，执行完成了，随时可以提交。
-  执行器收到通知后记录binlog，然后调用引擎接口，提交redo log 为提交状态。
-  更新完成。

 

这里肯定有同学会问，为什么要用两个日志模块，用一个日志模块不行吗？这就是之前mysql的模式了，MyISAM引擎是没有redo log的，那么我们知道它是不支持事务的，所以并不是说只用一个日志模块不可以，只是InnoDB引擎就是通过redo log来支持事务的。那么，又会有同学问，我用两个日志模块，但是不要这么复杂行不行，为什么redo log 要引入prepare预提交状态？这里我们用反证法来说明下为什么要这么做？

-  **先写redo log 直接提交，然后写 binlog**，假设写完redo log 后，机器挂了，binlog日志没有被写入，那么机器重启后，这台机器会通过redo log恢复数据，但是这个时候bingog并没有记录该数据，后续进行机器备份的时候，就会丢失这一条数据，同时主从同步也会丢失这一条数据。
-  **先写binlog，然后写redo log**，假设写完了binlog，机器异常重启了，由于没有redo log，本机是无法恢复这一条记录的，但是binlog又有记录，那么和上面同样的道理，就会产生数据不一致的情况。

如果采用**redo log 两阶段提交的方式就不一样了，写完binglog后，然后再提交redo log**就会防止出现上述的问题，从而保证了数据的一致性。那么问题来了，有没有一个极端的情况呢？假设redo log 处于预提交状态，binglog也已经写完了，这个时候发生了异常重启会怎么样呢？ 这个就要依赖于mysql的处理机制了，mysql的处理过程如下：

-  判断redo log 是否完整，如果判断是完整的，就立即提交。
-  如果redo log 只是预提交但不是commit状态，这个时候就会去判断binlog是否完整，如果完整就提交 redo log, 不完整就回滚事务。

这样就解决了数据一致性的问题。

[![img](https://img2020.cnblogs.com/blog/1365470/202003/1365470-20200330144113995-393835380.png)](https://img2020.cnblogs.com/blog/1365470/202003/1365470-20200330144113995-393835380.png)

# 25.InnoDB引擎中的索引策略，了解过吗？

## 覆盖索引

**覆盖索引是指在普通索引树中可以得到查询的结果，不需要在回到主键索引树中再次搜索**。

建立如下这张表来演示覆盖索引：

```sql
mysql> create table T (
ID int primary key,
age int NOT NULL DEFAULT 0, 
name varchar(16) NOT NULL DEFAULT '',
index age(age))
engine=InnoDB;
复制代码
```

我们执行`select * from T where age between 13 and 25`  语句，这条语句的执行流程大概为：

- 1、在 age 索引树中查找到 age = 13 的记录，取得 ID 的值
- 2、根据 id 的值在主键索引上查找所需要的所有信息
- 3、在 age 树上往下取，重复 1、2 两步操作，直到 age 不符合条件为止。

如果我们将语句换为 `select ID from T where age between 13 and 25`，执行这条语句时，在 age 索引树上就可以查询到 ID 的值，省去了上面的回表操作，这样就减少了搜索次数，提升了查询效率。

这时候的 age 索引树已经可以满足我们的查询需求，age 索引就称为覆盖索引。

覆盖索引是常用的数据查询优化技术，可以极大的提升数据库性能，有以下几个原因：

- **减少树的搜索次数，显著提升查询性能**。
- **索引是按照值的顺序存储，所以对于 I/O 密集型的范围查询比随机从磁盘中读取每一行的 I/O 要少很多**。
- **索引的条目远小于数据的条目，在索引树上读取会极大的减小数据库的访问量**。

## 最左前缀原则

**最左前缀原则是建立在联合索引之上的，如果我们建立了联合索引，我们不需要使用索引的全部定义，只要用到了索引中的最左边的那个字段就可以使用这个索引，这就是 B-tree 索引支持最左前缀原则。**

建立如下这张表来解释最左前缀原则：

```sql
mysql> create table T (
ID int primary key,
age int NOT NULL DEFAULT 0, 
name varchar(16) NOT NULL DEFAULT '',
ismale tinyint(1) DEFAULT NULL,
email varchar(64),
address varchar(255),
KEY `name_age` (`name`,`age`))
engine=InnoDB;
复制代码
```

我们建立了联合索引 name_age，现在，假设我们有一下三种查询情景：

- 1、查出用户名的第一个字是“张”开头的人的年龄。即查询条件子句为"where name like '张%'"
- 2、查处用户名中含有“张”字的人的年龄。即查询条件子句为"where name like '%张%'"
- 3、查出用户名以“张”字结尾的人的年龄。即查询条件子句为"where name like '%张'"

在这三种情况中，第一种情况可以利用到 name_age 这个联合索引，加速查询，可以看出，我们并没有 使用索引的全部定义，**只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。**

如果我们将索引的顺序调整为`KEY`name_age`(`age`,`name`)` ，那么上面三种情况都使用不到这个联合索引。

**维护索引需要代价，所以有时候我们可以利用“最左前缀”原则减少索引数量**。

## 索引下推

**索引下推优化是 MySQL 5.6 引入的， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。**

建立如下这张表来解释索引下推：

```sql
mysql> create table T (
ID int primary key,
age int NOT NULL DEFAULT 0, 
name varchar(16) NOT NULL DEFAULT '',
ismale tinyint(1) DEFAULT NULL,
email varchar(64),
address varchar(255),
KEY `name_age` (`name`,`age`))
engine=InnoDB;
复制代码
```

在表中建立了 name、age 的联合索引，我们执行 `select * from T where name like '张%' and age=10 and ismale=1;`语句，**我们已经知道了B-tree 索引的最左前缀原则，所以将会用到 name_age 索引，因为索引下推优化，会在 name_age 索引树上判断 name 和 age 是否满足**。

根据我们上面的执行语句，会在 name_age 索引树上查找 name 以 '张' 开头的并且 age = 10 的数据，然后在回到主键索引树中查询所需要的信息，并不是所有 name_age 索引树上查找 name 以 '张' 开头的数据都回主键索引树中查询数据，这样就减少了一些不必要的查询。

假设我们的数据如下图所示：

![索引下推](https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/1/29/16feeb3c10e77b59~tplv-t2oaga2asx-zoom-in-crop-mark:4536:0:0:0.image)



在 name_age 索引树中有四条符合 name 以 '张'开头的数据，如果没有索引下推，则需要回到主键索引树上判断 age 是否等于 10 ，这样就需要回表四次，而有了索引下推之后，在 name_age 索引树上就判断 age 是否等于 10 ，只需要回表两次，这样就减少了回表次数，提升了查询性能。

# 26.数据库存储日期格式时，如何考虑时区转换问题？

datetime类型适合用来记录数据的原始的创建时间，修改记录中其他字段的值，datetime字段的值不会改变，除非手动修改它。

timestamp类型适合用来记录数据的最后修改时间，只要修改了记录中其他字段的值，timestamp字段的值都会被自动更新。

# 27.一条sql执行过长的时间，你如何优化，从哪些方面入手？

```js
查看是否涉及多表和子查询，优化Sql结构，如去除冗余字段，是否可拆表等
优化索引结构，看是否可以适当添加索引
数量大的表，可以考虑进行分离/分表（如交易流水表）
数据库主从分离，读写分离
explain分析sql语句，查看执行计划，优化sql
查看mysql执行日志，分析是否有其他方面的问题
```

# 28.MySQL性能分析常用指令

## 查看INSERT、UPDATE、DELETE、SELECT的执行频率

show status like '参数'

Version:0.9 StartHTML:0000000105 EndHTML:0000001510 StartFragment:0000000141 EndFragment:0000001470



Connections：连接MySQL服务器的次数。 • Uptime：MySQL服务器的上

线时间。 • Slow_queries：慢查询的次数。 • Innodb_rows_read：Select查询返回的行数 • 

Innodb_rows_inserted：执行INSERT操作插入的行数 • Innodb_rows_updated：执行UPDATE操作更新的

行数 • Innodb_rows_deleted：执行DELETE操作删除的行数 • Com_select：查询操作的次数。 • 

Com_insert：插入操作的次数。对于批量插入的 INSERT 操作，只累加一次。 • Com_update：更新操作

的次数。 • Com_delete：删除操作的次数。

## 慢查询日志

## **慢查询日志分析工具：mysqldumpslow**

## show profiles 查看SQL各执行阶段

## **统计SQL的查询成本：last_query_cost**

## **SHOW WARNINGS的使用**

## **分析优化器执行计划：trace**

```mysql
select * from information_schema.optimizer_trace\G
```

Version:0.9 StartHTML:0000000105 EndHTML:0000000337 StartFragment:0000000141 EndFragment:0000000297

## **MySQL监控分析视图-sys schema**

**1. 主机相关：**以host_summary开头，主要汇总了IO延迟的信息。

**2. Innodb相关：**以innodb开头，汇总了innodb buffer信息和事务等待innodb锁的信息。

**3. I/o相关：**以io开头，汇总了等待I/O、I/O使用量情况。

**4. 内存使用情况：**以memory开头，从主机、线程、事件等角度展示内存的使用情况

**5. 连接与会话信息：**processlist和session相关视图，总结了会话相关信息。

**6. 表相关：**以schema_table开头的视图，展示了表的统计信息。

**7. 索引信息：**统计了索引的使用情况，包含冗余索引和未使用的索引情况。

**8. 语句相关：**以statement开头，包含执行全表扫描、使用临时表、排序等的语句信息。

**9. 用户相关：**以user开头的视图，统计了用户使用的文件I/O、执行语句统计信息。

**10. 等待事件相关信息：**以wait开头，展示等待事件的延迟情况

# 29.Blob和text有什么区别

EXT与BLOB的主要差别就是**BLOB保存二进制数据，TEXT保存字符数据**。 目前几乎所有博客内容里的图片都不是以二进制存储在数据库的，而是把图片上传到服务器然后正文里使用标签引用，这样的博客就可以使用TEXT类型。 而BLOB就可以把图片换算成二进制保存到数据库中

# 30.mysql里记录货币用什么字段类型比较好？

货币在数据库中MySQL常用Decimal和Numric类型表示，这两种类型被MySQL实现为同样的类型。他们被用于保存与金钱有关的数据。

# 31.Mysql中有哪几种锁，列举一下？

按锁的操作类型分为两类

排他锁，共享锁

按锁的粒度划分

行锁  记录锁 间隙锁 临键锁 插入意向锁 

页锁

表锁 表级排他锁,表级共享锁，自增锁,意向锁，mdl锁

按多锁态度划分 

悲观锁

乐观锁

按加锁方式划分

隐式锁 显示锁

其它锁

全局锁，死锁

# 32.Hash索引和B+树区别是什么？你在设计索引是怎么抉择的？

B+树可以进行范围查询，Hash索引不能。
B+树支持联合索引的最左匹配原则，Hash索引不支持。
B+树支持order by排序，Hash索引不支持
Hash索引在等值查询上比B+树效率更高
B+树使用like进行模糊查询的时候、like后面比如（%开头）的话可以起到优化的作用，Hash索引根本无法进行模糊查询。

# 33.mysql 的内连接、左连接、右连接有什么区别？

## 一、内连接

关键字：inner join on

语句：select * from a_table a inner join b_table b on a.a_id = b.b_id;

说明：组合两个表中的记录，返回关联字段相符的记录，也就是返回两个表的交集部分。

## 二、左连接

关键字：left join on / left outer join on

语句：SELECT * FROM a_table a left join b_table b ON a.a_id = b.b_id;

说明： left join 是left outer join的简写，它的全称是左外连接，是外连接中的一种。 左(外)连接，左表(a_table)的记录将会全部表示出来，而右表(b_table)只会显示符合搜索条件的记录。右表记录不足的地方均为NULL。

## 三、右连接

关键字：right join on / right outer join on

语句：SELECT * FROM a_table a right outer join b_table b on a.a_id = b.b_id;

说明：right join是right outer join的简写，它的全称是右外连接，是外连接中的一种。与左(外)连接相反，右(外)连接，左表(a_table)只会显示符合搜索条件的记录，而右表(b_table)的记录将会全部表示出来。左表记录不足的地方均为NULL。

## 四、全连接

关键字：union / union all

语句：(select colum1,colum2...columN from tableA ) union (select colum1,colum2...columN from tableB )

或 (select colum1,colum2...columN from tableA ) union all (select colum1,colum2...columN from tableB )；

union语句注意事项：

　　1.通过union连接的SQL它们分别单独取出的列数必须相同；

　　2.不要求合并的表列名称相同时，以第一个sql 表列名为准；

　　3.使用union 时，完全相等的行，将会被合并，由于合并比较耗时，一般不直接使用 union 进行合并，而是通常采用union all 进行合并；

　　4.被union 连接的sql 子句，单个子句中不用写order by ，因为不会有排序的效果。但可以对最终的结果集进行排序；

　　　　(select id,name from A order by id) union all (select id,name from B order by id); //没有排序效果

　　　　(select id,name from A ) union all (select id,name from B ) order by id; //有排序效果

## 五、区别

　　1.内连接，显示两个表中有联系的所有数据；

　　2.左链接，以左表为参照,显示所有数据；

　　3.右链接，以右表为参照显示数据；

　　4.全连接，显示两个表中所有数据，除被合并的列以外元素值相等只显示一行数据，值不等显示多行数据。

# 34.说说MySQL 的基础架构图

第一层负责连接处理，授权认证，安全等等
第二层负责编译并优化SQL
第三层是存储引擎。

# 35.什么是 内连接、外连接、交叉连接、笛卡尔积等?

内连接: 只连接匹配的行

左外连接: 包含左边表的全部行（不管右边的表中是否存在与它们匹配的行），以及右边表中全部匹配的行

右外连接: 包含右边表的全部行（不管左边的表中是否存在与它们匹配的行），以及左边表中全部匹配的行

例如1：SELECT a.,b. FROM luntan LEFT JOIN usertable as b ON a.username=b.username

例如2：SELECT a.,b. FROM city as a FULL OUTER JOIN user as b ON a.username=b.username

全外连接: 包含左、右两个表的全部行，不管另外一边的表中是否存在与它们匹配的行。

交叉连接: 生成笛卡尔积－它不使用任何匹配或者选取条件，而是直接将一个数据源中的每个行与另一个数据源的每个行都一一匹配

# 36.说一下数据库的三大范式

## 范式一

属性是原子性的，不可再分隔

**1．第一范式(确保每列保持原子性)**

第一范式是最基本的范式。如果数据库表中的所有字段值都是不可分解的原子值，就说明该数据库表满足了第一范式。

## 范式二

1NF 告诉我们字段属性需要是原子性的，而 2NF 告诉我们一张表就是一个独立的对象，一张表只表达一个意思。

**第二范式(确保表中的每列都和主键相关)**

第二范式在第一范式的基础之上更进一层。第二范式需要确保数据库表中的每一列都和主键相关，而不能只与主键的某一部分相关（主要针对联合主键而言）。也就是说在一个数据库表中，一个表中只能保存一种数据，不可以把多种数据保存在同一张数据库表中。

## 范式三

每个非键属性依赖于键，依赖于整个键，并且除了键别无他物”。

**第三范式(确保每列都和主键列直接相关,而不是间接相关)**

第三范式需要确保数据表中的每一列数据都和主键直接相关，而不能间接相关。

比如在设计一个订单数据表的时候，可以将客户编号作为一个外键和订单表建立相应的关系。而不可以在订单表中添加关于客户其它信息（比如姓名、所属公司等）的字段。如下面这两个表所示的设计就是一个满足第三范式的数据库表。

## 巴式范式

**在 3NF 的基础上消除了主属性对候选键的部分依赖或者传递依赖关系**。

## 第四范式

去除多值依赖 导致的异常

## 第五范式

消除不是由候选键所蕴含的连接依赖。**如果关系模式R中的每一个连**

**接依赖均由R的候选键所隐含**，则称此关系模式符合第五范式。

# 37.mysql有关权限的表都有哪几个？

MySQL服务器通过权限表来控制用户对数据库的访问，权限表存放在mysql数据库里，由mysql_install_db脚本初始化。这些权限表分别user，db，table_priv，columns_priv和host。



下面分别介绍一下这些表的结构和内容：



user权限表：记录允许连接到服务器的用户帐号信息，里面的权限是全局级的。



db权限表：记录各个帐号在各个数据库上的操作权限。



table_priv权限表：记录数据表级的操作权限。



columns_priv权限表：记录数据列级的操作权限。



host权限表：配合db权限表对给定主机上数据库级操作权限作更细致的控制。这个权限表不受GRANT和REVOKE语句的影响。

# 38.MySQL的binlog有有几种录入格式？分别有什么区别？

**有三种格式，statement，row和mixed。**

- statement模式下，每一条会修改数据的sql都会记录在binlog中。不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。由于sql的执行是有上下文的，因此在保存的时候需要保存相关的信息，同时还有一些使用了函数之类的语句无法被记录复制。
- row级别下，不记录sql语句上下文相关信息，仅保存哪条记录被修改。记录单元为每一行的改动，基本是可以全部记下来但是由于很多操作，会导致大量行的改动(比如alter table)，因此这种模式的文件保存的信息太多，日志量太大。
- mixed，一种折中的方案，普通操作使用statement记录，当无法使用statement的时候使用row。 此外，新版的MySQL中对row级别也做了一些优化，当表结构发生变化的时候，会记录语句而不是逐行记录。

# 39.InnoDB引擎的4大特性，了解过吗

## **插入缓冲** 

插入缓存之前版本叫insert buffer，现版本 change buffer，主要提升插入性能，change buffer是insert buffer的加强，insert buffer只针对insert有效，change buffering对insert、delete、update(delete+insert)、purge都有效。有什么用呢？

对于非聚集索引来说，比如存在用户购买金额这样一个字段，索引是普通索引，每个用户的购买的金额不相同的概率比较大，这样导致可能出现购买记录在数据在数据里的排序可能是1000，3，499，35…，这种不连续的数据，一会插入这个数据页，一会插入那个数据页，这样造成的IO是很耗时的，所以出现了Insert Buffer。

Insert Buffer是怎么做的呢？mysql对于非聚集索引的插入，先去判断要插入的索引页是否已经在内存中了，如果不在，暂时不着急先把索引页加载到内存中，而是把它放到了一个Insert Buffer对象中，临时先放在这，然后等待情况，等待很多和现在情况一样的非聚集索引，再和要插入的非聚集索引页合并，比如说现在Insert Buffer中有1，99，2，100，合并之前可能要4次插入，合并之后1，2可能是一个页的，99，100可能是一个页的，这样就减少到了2次插入。这样就提升了效率和插入性能，减少了随机IO带来性能损耗。

综合上述，Insert Buffer 只对于非聚集索引（非唯一）的插入和更新有效，对于每一次的插入不是写到索引页中，而是先判断插入的非聚集索引页是否在缓冲池中，如果在则直接插入；若不在，则先放到Insert Buffer 中，再按照一定的频率进行合并操作，再写回disk。这样通常能将多个插入合并到一个操作中，目的还是减少了随机IO带来性能损耗。

使用插入缓冲的条件：

非聚集索引
非唯一索引
innodb_change_buffer设置的值有：

all: 默认值，缓存insert, delete, purges操作
none: 不缓存
inserts: 缓存insert操作
deletes: 缓存delete操作
changes: 缓存insert和delete操作
purges: 缓存后台执行的物理删除操作
可以通过参数控制其使用的大小：

mysql> show variables like 'innodb_change_buffer_max_size';
+-------------------------------+-------+
| Variable_name                 | Value |
+-------------------------------+-------+
| innodb_change_buffer_max_size | 25    |
+-------------------------------+-------+
 row in set (0.05 sec)

innodb_change_buffer_max_size，默认是25%，即缓冲池的1/4。最大可设置为50%。当MySQL实例中有大量的修改操作时，要考虑增大innodb_change_buffer_max_size

上面提过在一定频率下进行合并，那所谓的频率是什么条件？
1）辅助索引页被读取到缓冲池中。正常的select先检查Insert Buffer是否有该非聚集索引页存在，若有则合并插入。

2）辅助索引页没有可用空间。空间小于1/32页的大小，则会强制合并操作。

3）Master Thread 每秒和每10秒的合并操作。

## **双写机制（Double Write）**

在InnoDB将BP中的Dirty Page刷（flush）到磁盘上时，首先会将（memcpy函数）Page刷到InnoDB tablespace的一个区域中，我们称该区域为Double write Buffer（大小为2MB，每次写入1MB，128个页，每个页16k,其中120个页为后台线程的批量刷Dirty Page，还有8个也是为了前台起的sigle Page Flash线程，用户可以主动请求，并且能迅速的提供空余的空间）。在向Double write Buffer写入成功后，第二步、再将数据分别刷到一个共享空间和真正应该存在的位置。

MySQL可以根据redolog进行恢复,而mysq在恢复的过程中是检查page"的checksum, checksum就是pgae的最后事务号,发生partial page write问题时. DageR经损坏,找不到该page中的事务号就无法恢复。

具体的流程如下图所示：


在不同的写入阶段，操作系统crash后，double write带来的保护机制：



阶段一：copy过程中，操作系统crash，重启之后，脏页未刷到磁盘，但更早的数据并没有发生损坏，重新写入即可

阶段二：write到共享表空间过程中，操作系统crash，重启之后，脏页未刷到磁盘，但更早的数据并没有发生损坏，重新写入即可

阶段三：write到独立表空间过程中，操作系统crash，重启之后，发现：
（1）数据文件内的页损坏：头尾checksum值不匹配（即出现了partial page write的问题）。从共享表空间中的doublewrite segment内恢复该页的一个副本到数据文件，再应用redo log；
（2）若页自身的checksum匹配，但与doublewrite segment中对应页的checksum不匹配，则统一可以通过apply redo log来恢复。

阶段X：recover过程中，操作系统crash，重启之后，innodb面对的情况同阶段三一样（数据文件损坏，但共享表空间内有副本），再次应用redo log即可。

## 适应哈希索引（Adaptive Hash Index，AHI）

哈希算法是一种非常快的查找方法，在一般情况（没有发生hash冲突）下这种查找的时间复杂度为O(1)。InnoDB存储引擎会监控对表上辅助索引页的查询。如果观察到建立hash索引可以提升性能，就会在缓冲池建立hash索引，称之为自适应哈希索引（Adaptive Hash Index，AHI）

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210330151915701.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTMyMDY2MA==,size_16,color_FFFFFF,t_70)

自适应哈希索引由innodb_adaptive_hash_index 变量启用,AHI是通过缓冲池的B+ Tree构造而来，使用索引键的前缀来构建哈希索引，前缀可以是任意长度。InnoDB存储引擎会自动根据访问的频率和模式来自动地为某些热点页建立hash索引。加快索引读取的效果，相当于索引的索引，帮助InnoDB快速读取索引页。

根据InnoDB官方文档说明，启用了AHI后，读写的速度会提升2倍，辅助索引的连接操作性能可以提高5倍。

查看AHI的工作状态：

```mysql
show engine innodb status;
```

Hash table size：代表AHI的大小；
hash searches/s：代表命中hash查询的频率；
non-hash searches/s：代表没有命中hash查询的频率；
注意：hash查询是等值查询，例如模糊查询、范围查找，是不能使用hash索引的。用户可以根据实际场景去权衡是否要开启AHI。

## 预读 （Read Ahead）

预读（read-ahead)操作是一种IO操作，用于异步将磁盘的页读取到buffer pool中，预料这些页会马上被读取到。预读请求的所有页集中在一个范围内。InnoDB使用两种预读算法：

Linear read-ahead：线性预读技术预测在buffer pool中被访问到的数据它临近的页也会很快被访问到。能够通过调整被连续访问的页的数量来控制InnoDB的预读操作，使用参数 innodb_read_ahead_threshold配置，添加这个参数前，InnoDB会在读取到当前区段最后一页时才会发起异步预读请求
innodb_read_ahead_threshold 这个参数控制InnoDB在检测顺序页面访问模式时的灵敏度。如果在一个区块顺序读取的页数大于或者等于 innodb_read_ahead_threshold 这个参数，InnoDB启动预读操作来读取下一个区块。innodb_read_ahead_threshold参数值的范围是 0-64，默认值为56. 这个值越高则访问默认越严格。比如，如果设置为48，在当前区块中当有48个页被顺序访问时，InnoDB就会启动异步的预读操作，如果设置为8，则仅仅有8个页被顺序访问就会启动异步预读操作。你可以在MySQL配置文件中设置这个值，或者通过SET GLOBAL 语句动态修改（需要有set global 权限）。

Random read-ahead: 随机预读通过buffer pool中存中的来预测哪些页可能很快会被访问，而不考虑这些页的读取顺序。如果发现buffer pool中存中一个区段的13个连续的页，InnoDB会异步发起预读请求这个区段剩余的页。通过设置 innodb_random_read_ahead 为 ON开启随机预读特性。
通过 SHOW INNODB ENGINE STATUS 命令输出的统计信息可以帮助你评估预读算法的效果，统计信息包含了下面几个值：

innodb_buffer_pool_read_ahead   通过预读异步读取到buffer pool的页数
innodb_buffer_pool_read_ahead_evicted 预读的页没被使用就被驱逐出buffer pool的页数，这个值与上面预读的页数的比值可以反应出预读算法的优劣。
innodb_buffer_pool_read_ahead_rnd  由InnoDB触发的随机预读次数。

# 39.索引有哪些优缺点？

## 索引的优点

① 建立索引的列可以保证行的唯一性，生成唯一的rowId

② 建立索引可以有效缩短数据的检索时间

③ 建立索引可以加快表与表之间的连接

④ 为用来排序或者是分组的字段添加索引可以加快分组和排序顺序

## 索引的缺点

① 创建索引和维护索引需要时间成本，这个成本随着数据量的增加而加大

② 创建索引和维护索引需要空间成本，每一条索引都要占据数据库的物理存储空间，数据量越大，占用空间也越大（数据表占据的是数据库的数据空间）

③ 会降低表的增删改的效率，因为每次增删改索引需要进行动态维护，导致时间变长

# 40.索引有哪几种类型？

按数据结构分类可分为：**B+tree索引、Hash索引、Full-text索引**。
按物理存储分类可分为：**聚簇索引、二级索引（辅助索引）**。
按字段特性分类可分为：**主键索引、普通索引、前缀索引**。
按字段个数分类可分为：**单列索引、联合索引（复合索引、组合索引）**。

# 41.创建索引有什么原则呢？

## 1．选择唯一性索引


唯一性索引的值是唯一的，可以更快速的通过该索引来确定某条记录。例如，学生表中学号是具有唯一性的字段。为该字段建立唯一性索引可以很快的确定某个学生的信息。如果使用姓名的话，可能存在同名现象，从而降低查询速度。


## 2．为经常需要排序、分组和联合操作的字段建立索引


经常需要ORDER BY、GROUP BY、DISTINCT和UNION等操作的字段，排序操作会浪费很多时间。如果为其建立索引，可以有效地避免排序操作。


## 3．为常作为查询条件的字段建立索引


如果某个字段经常用来做查询条件，那么该字段的查询速度会影响整个表的查询速度。因此，为这样的字段建立索引，可以提高整个表的查询速度。


## 4．限制索引的数目


索引的数目不是越多越好。每个索引都需要占用磁盘空间，索引越多，需要的磁盘空间就越大。修改表时，对索引的重构和更新很麻烦。越多的索引，会使更新表变得很浪费时间。


## 5．尽量使用数据量少的索引


如果索引的值很长，那么查询的速度会受到影响。例如，对一个CHAR(100)类型的字段进行全文检索需要的时间肯定要比对CHAR(10)类型的字段需要的时间要多。


## 6．尽量使用前缀来索引


如果索引字段的值很长，最好使用值的前缀来索引。例如，TEXT和BLOG类型的字段，进行全文检索会很浪费时间。如果只检索字段的前面的若干个字符，这样可以提高检索速度。


## 7．删除不再使用或者很少使用的索引


表中的数据被大量更新，或者数据的使用方式被改变后，原有的一些索引可能不再需要。[数据库](http://www.2cto.com/database/)管理员应当定期找出这些索引，将它们删除，从而减少索引对更新操作的影响。


注意：选择索引的最终目的是为了使查询的速度变快。上面给出的原则是最基本的准则，但不能拘泥于上面的准则。读者要在以后的学习和工作中进行不断的实践。根据应用的实际情况进行分析和判断，选择最合适的索引方式。

## **8. 在where中使用不到的字段，不要设置索引** 

## **9. 数据量小的表最好不要使用索引**

## 10.**有大量重复数据的列上不要建立索引**

## **11. 避免对经常更新的表创建过多的索引** 

## **12. 不建议用无序的值作为索引**

## **13.使用最频繁的列放到联合索引的左侧**

## **14.在多个字段都要创建索引的情况下，联合索引优于单值索引**

## **15.多表 JOIN 连接操作时，创建索引注意事项**

## 16.表经常进行更新的字段

# 42.创建索引的三种方式

1.创建索引时进行指定

2.create index xxx on 表(字段);

3.alter table xxx add index xxx(字段);

# 43 .百万级别或以上的数据，你是如何删除的

1. 删除索引
2. 删除需要删除的数据
3. 重新建立索引

# 44.什么是最左前缀原则？什么是最左匹配原则？

最左前缀原则：顾名思义是最左优先，以最左边的为起点任何连续的索引都能匹配上。

# 45.B树和B+树的区别，数据库为什么使用B+树而不是B树？

首先，B+树查询效率更稳定。因为B+树每次只有访问到叶子节点才能找到对应的数据，而在B树中，非叶子节
点也会存储数据，这样就会造成查询效率不稳定的情况，有时候访问到了非叶子节点就可以找到关键字，而有时
需要访问到叶子节点才能找到关键字。

其次，B+树的查询效率更高。这是因为通常B+树比B树更矮胖（阶数更大，深度更低），查询所需要的磁盘
io也会更少。同样的磁盘页大小，B+树可以存储更多的节点关键字。

不仅是对单个关键字的查询上，在查询范围上，B+树的效率也比B树高。这是因为所有关键字都出现在B+树的
叶子节点中，叶子节点之间会有指针，数据又是递增的，这使得我们范围查找可以通过指针连接查找。而在B树
中则需要通过中序遍历才能完成查询范围的查找，效率要低很多

# 46.覆盖索引、回表等这些，了解过吗？

覆盖索引： 查询列要被所建的索引覆盖，不必从数据表中读取，换句话说查询列要被所使用的索引覆盖。

回表：二级索引无法直接查询所有列的数据，所以通过二级索引查询到聚簇索引后，再查询到想要的数据，这种通过二级索引查询出来的过程，就叫做回表。

# 47.B+树在满足聚簇索引和覆盖索引的时候不需要回表查询数据?

在B+树的索引中，叶子节点可能存储了当前的key值，也可能存储了当前的key值以及整行的数据，这就是聚簇索引和非聚簇索引。 在InnoDB中，只有主键索引是聚簇索引，如果没有主键，则挑选一个唯一键建立聚簇索引。如果没有唯一键，则隐式的生成一个键来建立聚簇索引。

当查询使用聚簇索引时，在对应的叶子节点，可以获取到整行数据，因此不用再次进行回表查询

# 48.何时使用聚簇索引与非聚簇索引

![img](https://ask.qcloudimg.com/http-save/yehe-2823867/rcil81aoyd.jpeg?imageView2/2/w/2560/h/7000)

# 49.非聚簇索引一定会回表查询吗？

不一定，出现了索引覆盖的情况下不需要回表，即我们的查询的数据在我们的非聚簇都能找到时

# 50.组合索引是什么？为什么需要注意组合索引中的顺序？

组合索引，用户可以在多个列上建立索引,这种索引叫做组合索引。

因为InnoDB引擎中的索引策略的最左原则，所以需要注意组合索引中的顺序。

# 51.什么是数据库事务？

数据库事务( transaction)是访问并可能操作各种[数据项](https://baike.baidu.com/item/数据项/3227309?fromModule=lemma_inlink)的一个数据库操作[序列](https://baike.baidu.com/item/序列/1302588?fromModule=lemma_inlink)，这些操作要么全部执行,要么全部不执行，是一个不可分割的工作单位。事务由事务开始与事务结束之间执行的全部数据库操作组成。

# 52.隔离级别与锁的关系

1.在Read Uncommitted级别下，读取数据不需要加共享锁，这样就不会跟被修改的数据上的排他锁冲突;
2.在Read Committed级别下，读操作需要加共享锁，但是在语句执行完以后释放共享锁；
3.在Repeatable Read级别下，读操作需要加共享锁，但是在事务提交之前并不释放共享锁，也就是必须等待事务执行完毕以后才释放共享锁。
4.SERIALIZABLE 是限制性最强的隔离级别，因为该级别锁定整个范围的键，并一直持有锁，直到事务完成。

# 53.按照锁的粒度分，数据库锁有哪些呢？锁机制与InnoDB锁算法

# 54.按照锁的粒度分，数据库锁有哪些呢？锁机制与InnoDB锁算法

行锁

页锁

表锁

- innodb对于行的查询使用next-key lock
- Next-locking keying为了解决Phantom Problem幻读问题
- 当查询的索引含有唯一属性时，将next-key lock降级为record key
- Gap锁设计的目的是为了阻止多个事务将记录插入到同一范围内，而这会导致幻读问题的产生
- 有两种方式显式关闭gap锁：（除了外键约束和唯一性检查外，其余情况仅使用record lock） A. 将事务隔离级别设置为RC B. 将参数innodb_locks_unsafe_for_binlog设置为1

# 55.从锁的类别角度讲，MySQL都有哪些锁呢？

共享锁和排他锁

# 56.MySQL中InnoDB引擎的行锁是怎么实现的？

InnoDB行锁是通过给索引上的索引项加锁来实现的，这一点MySQL与Oracle不同，后者是通过在数据块中对相应数据行加锁来实现的。InnoDB这种行锁实现特点意味着：只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁！

# 57.为什么要使用视图？什么是视图？

视图实际上是代表了一段sql查询语句，可以理解成视图是一张虚拟的表，表中的数据会随着原表的改变而改变。

## 1.可以定制用户数据，聚焦特定的数据

## 2.可以简化数据操作

## 3.基表中的数据就有了一定的安全性

## 4.可以合并分离的数据，创建分区视图

# 58.视图有哪些特点？哪些使用场景？

视图特点：

```js
视图的列可以来自不同的表，是表的抽象和在逻辑意义上建立的新关系。
视图是由基本表(实表)产生的表(虚表)。
视图的建立和删除不影响基本表。
对视图内容的更新(添加，删除和修改)直接影响基本表。
当视图来自多个基本表时，不允许添加和删除数据。
```

视图用途： 简化sql查询，提高开发效率，兼容老的表结构。

视图的常见使用场景：

```js
重用SQL语句；
简化复杂的SQL操作。
使用表的组成部分而不是整个表；
保护数据
更改数据格式和表示。视图可返回与底层表的表示和格式不同的数据。
```

# 59.视图的优点，缺点，讲一下？

# 优点

使用场景

## 缺点

1)性能差　　 

   sql server必须把视图查询转化成对基本表的查询，如果这个视图是由一个复杂的多表查询所定义，那么，即使是视图的一个简单查询，sql server也要把它变成一个复杂的结合体，需要花费一定的时间。 

　2)修改限制　　 

   当用户试图修改试图的某些信息时，数据库必须把它转化为对基本表的某些信息的修改，对于简单的试图来说，这是很方便的，但是，对于比较复杂的试图，可能是不可修改的。

# 60.执行count(1)、count(*) 与 count(列名) 到底有什么区别？

**1. count(1) and count(\*)**

从执行计划来看，count(1)和count(*)的效果是一样的。

**当表的数据量大些时**，对表作分析之后，使用count(1)还要比使用count(*)用时多！ 当数据量在1W以内时，count(1)会比count(*)的用时少些，不过也差不了多少。

如果count(1)是聚集索引时，那肯定是count(1)快，但是差的很小。 因为count(*)，会自动优化指定到那一个字段。所以没必要去count(1)，使用count(*)，sql会帮你完成优化的 因此：在有聚集索引时count(1)和count(*)基本没有差别！

**2. count(1) and count(字段)**

两者的主要区别是

- count(1) 会统计表中的所有的记录数，包含字段为null 的记录。
- count(字段) 会统计该字段在表中出现的次数，忽略字段为null 的情况。
  即不统计字段为null 的记录。

**3. count(\*) 和 count(1)和count(列名)区别**

执行效果上：

- count(*)包括了所有的列，相当于行数，在统计结果的时候，**不会忽略为NULL的值。**
- count(1)包括了忽略所有列，用1代表代码行，在统计结果的时候，**不会忽略为NULL的值**。
- count(列名)只包括列名那一列，在统计结果的时候，会忽略列值为空（这里的空不是指空字符串或者0，而是表示null）的计数，**即某个字段值为NULL时，不统计**。

执行效率上：

- 列名为主键，count(列名)会比count(1)快
- 列名不为主键，count(1)会比count(列名)快
- 如果表多个列并且没有主键，则 count(1 的执行效率优于 count（*）
- 如果有主键，则 select count（主键）的执行效率是最优的
- 如果表只有一个字段，则 select count（*）最优。

# 61.什么是游标？

游标是一种临时的数据库对象，可以指向存储在数据库表中的数据行指针。这里游标 充当了 

指针的作用 ，我们可以通过操作游标来对数据行进行操作。

**游标让 SQL 这种面向集合的语言有了面向过程开发的能力**

# 62.什么是存储过程?有什么优点?

存储过程（Stored Procedure）是在[数据库](https://cloud.tencent.com/solution/database?from=10680)中，一组为了完成特定功能的SQL 语句集，它存储在数据库中，一次编译后永久有效，用户通过指定存储过程的名字并给出参数（可选）来执行

## 存储过程的优点

- 预编译SQL，提升执行效率
- 可以隐藏执行逻辑，只暴露名称和参数
- 相较于程序来说，修改起来更加便捷

## 存储过程的缺点

随着SQL行数的增加，维护复杂度呈线性提升

无法调试，迭代过程中风险较高

# 63.什么是触发器

触发器是用户定义在关系表上的一类由事件驱动的特殊的存储过程。触发器是指一段代码，当触发某个事件时，自动执行这些代码。

触发器是由 事件来触发 某个操作，这些事件包括 INSERT 、 UPDATE 、 DELETE 事件。所谓事件就是指

用户的动作或者触发某项行为。如果定义了触发程序，当数据库执行这些语句时候，就相当于事件发生

了，就会 自动 激发触发器执行相应的操作。

当对数据表中的数据执行插入、更新和删除操作，需要自动执行一些数据库逻辑时，可以使用触发器来

实现。

## 使用场景

可以通过数据库中的相关表实现级联更改。

实时监控某张表中的某个字段的更改而需要做出相应的处理。

例如可以生成某些业务的编号。

# 64.MySQL中都有哪些触发器？

## before Insert

## After Insert

## Before Update

## After Update

## Before Delete

## After Delete

# 65.超键、候选键、主键、外键分别是什么？

超键(super key):在关系中能唯一标识元组的属性集称为关系模式的超键

候选键(candidate key):不含有多余属性的超键称为候选键

主键(primary key):用户选作元组标识的一个候选键程序主键

外键(foreign key)如果关系模式R1中的某属性集不是R1的主键，而是另一个关系R2的主键则该属性集是关系模式R1的外键。

# 66.SQL 约束有哪几种呢？

主键约束,唯一约束,默认值约束，非空约束,检查约束，外键约束

# 67.谈谈六种关联查询，使用场景

交叉连接（CROSS JOIN）；内连接（INNER JOIN）；外连接（LEFT JOIN/RIGHT JOIN）；联合查询（UNION与UNION ALL）；全连接（FULL JOIN）；自连接（SLEF JOIN）

# 69.varchar(50)中50的涵义

最多存放50个字符

# 70.mysql中int(20)和char(20)以及varchar(20)的区别

int(20) 表示字段是int类型，显示长度是 20

char(20)表示字段是固定长度字符串，长度为 20

varchar(20) 表示字段是可变长度字符串，长度为 20

# 71.delete，drop，truncate 区别

delete，drop，truncate 都有删除表的作用，区别在于：

- 1、delete 和 truncate 仅仅删除表数据，drop 连表数据和表结构一起删除，打个比方，delete 是单杀，truncate 是团灭，drop 是把电脑摔了。
- 2、delete 是 DML 语句，操作完以后如果没有不想提交事务还可以回滚，truncate 和 drop 是 DDL 语句，操作完马上生效，不能回滚，打个比方，delete 是发微信说分手，后悔还可以撤回，truncate 和 drop 是直接扇耳光说滚，不能反悔。
- 3、执行的速度上，**drop>truncate>delete**，打个比方，drop 是神舟火箭，truncate 是和谐号动车，delete 是自行车。

# 72.union all和union的区别 和使用?

> 如果我们需要将**两个select语句的结果作为一个整体显示出来**，我们就需要用到[union](https://so.csdn.net/so/search?q=union&spm=1001.2101.3001.7020)或者union all关键字。
>
> union(或称为联合)的作用是将多个结果合并在一起显示出来。

- UNION用的比较多union all是直接连接，取到得是所有值，记录可能有重复
- union 是取唯一值，记录没有重复

## UNION和UNION ALL效率：

> UNION和UNION ALL关键字都是将两个结果集合并为一个，但这两者从使用和效率上来说都有所不同。
>
> 
>
> 1、**对重复结果的处理**：UNION在进行表链接后会筛选掉重复的记录，Union All不会去除重复记录。
>
> 
>
> 2、**对排序的处理**：Union将会按照字段的顺序进行排序；UNION ALL只是简单的将两个结果合并后就返回。
>
> 
>
> 从效率上说，UNION ALL 要比UNION快很多，所以，如果可以确认合并的两个结果集中不包含重复数据且不需要排序时的话，那么就使用UNION ALL。

# 73.sql的生命周期

1. 应用服务器与数据库服务器建立一个连接
2. 数据库进程拿到请求sql
3. 解析并生成执行计划，执行
4. 读取数据到内存并进行逻辑处理
5. 通过步骤一的连接，发送结果到客户端
6. 关掉连接，释放资源

74.一条Sql的执行顺序？

1. FROM子句组装数据（包括通过ON进行连接）；
2. WHERE子句进行条件筛选；
3. GROUP BY分组 ；
4. 使用聚集函数进行计算；
5. HAVING筛选分组；
6. 计算所有的表达式；
7. SELECT 的字段；
8. ORDER BY排序；
9. LIMIT筛选。

# 74.列值为NULL时，查询是否会用到索引？

MySQL 中存在 NULL 值的列也是走索引的

计划对列进行索引，应尽量避免把它设置为可空，因为这会让 MySQL 难以优化引用了可空列的查询，同时增加了引擎的复杂度

# 75.关心过业务系统里面的sql耗时吗？统计过慢查询吗？对慢查询都怎么优化过？

首先分析语句，看看是否load了额外的数据，可能是查询了多余的行并且抛弃掉了，可能是加载了许多结果中并不需要的列，对语句进行分析以及重写。 分析语句的执行计划，然后获得其使用索引的情况，之后修改语句或者修改索引，使得语句可以尽可能的命中索引。 如果对语句的优化已经无法进行，可以考虑表中的数据量是否太大，如果是的话可以进行横向或者纵向的分表。

# 76.主键使用自增ID还是UUID，为什么？

### 一、自增还是UUID？数据库主键的类型选择

　　自增还是UUID？这个问题看似简单，但是能诱发很多思考，也涉及到了很多细节。先说下uuid和 auto_increment（数据库自增主键）的优缺点吧，因为是个人理解，如有错误恳请指出：

1、自增主键

　　自增ID是在设计表时将id字段的值设置为自增的形式，这样当插入一行数据时无需指定id会自动根据前一字段的ID值+1进行填充。在MySQL数据库中，可通过sql语句AUTO_INCREMENT来对特定的字段启用自增赋值 使用自增ID作为主键，能够保证字段的原子性。

　　auto_increment优点：

- 字段长度较uuid小很多，可以是bigint甚至是int类型，这对检索的性能会有所影响。我们平时数据库一般用的都是innodb引擎的表，这种表格检索数据的时候，哪怕走索引，也是先根据索引找到主键，然后由主键找到这条记录。所以主键的长度短的话，读性能是会好一点的。
- 在写的方面，因为是自增的，所以主键是趋势自增的，也就是说新增的数据永远在后面，这点对于性能有很大的提升（这点我接下来会在uuid的优缺点分析中解释，虽然用词可能不太专业）
- 数据库自动编号，速度快，而且是增量增长，按顺序存放，对于检索非常有利；
- 数字型，占用空间小，易排序，在程序中传递也方便；
- 如果通过非系统增加记录时，可以不用指定该字段，不用担心主键重复问题。

　　auto_incremen的缺点：

- 最致命的一个缺点就是，很容易被别人知晓业务量，然后很容易被网络爬虫教做人
- 高并发的情况下，竞争自增锁会降低数据库的吞吐能力
- 数据迁移的时候，特别是发生表格合并这种操作的时候，会非常蛋疼

　　因为自动增长，在手动要插入指定ID的记录时会显得麻烦，尤其是当系统与其它系统集成时，需要数据导入时，很难保证原系统的ID不发生主键冲突（前提是老系统也是数字型的）。特别是在新系统上线时，新旧系统并行存在，并且是异库异构的数据库的情况下，需要双向同步时，自增主键将是你的噩梦；在系统集成或割接时，如果新旧系统主键不同是数字型就会导致修改主键数据类型，这也会导致其它有外键关联的表的修改，后果同样很严重；若系统也是数字型的，在导入时，为了区分新老数据，可能想在老数据主键前统一加一个字符标识（例如“o”，old）来表示这是老数据，那么自动增长的数字型又面临一个挑战。

2、UUID

　　UUID含义是通用唯一识别码 (Universally Unique Identifier)，指在一台机器上生成的数字，它保证对在同一时空中的所有机器都是唯一的。通常平台会提供生成的API。换句话说能够在一定的范围内保证主键id的唯一性。

　　优点：

- 地球唯一的guid，绝对不会冲突。数据拆分、合并存储的时候，能达到全局的唯一性
- 可以在应用层生成，提高数据库吞吐能力
- 是string类型，写代码的时候方便很多

　　缺点：

- 影响插入速度， 并且造成硬盘使用率低。与自增相比，最大的缺陷就是随机io。这一点又要谈到我们的innodb了，因为这个默认引擎，表中数据是按照主键顺序存放的。也就是说，如果发生了随机io，那么就会频繁地移动磁盘块。当数据量大的时候，写的短板将非常明显。当然，这个缺点可以通过nosql那些产品解决。
- uuid之间比较大小相对数字慢不少， 影响查询速度。
- uuid占空间大， 如果你建的索引越多， 影响越严重
- 读取出来的数据也是没有规律的，通常需要order by，其实也很消耗数据库资源
- 看起来比较丑

#  77.mysql自增主键用完了怎么办？

这问题没遇到过，因为自增主键我们用int类型，一般达不到最大值，就要考虑分表分库了。

mysql数据库表的自增 ID 达到上限之后，这时候再申请它的值就不会再改变了，如果继续插入数据就会导致报主键冲突异常。

因此在做数据字典设计时，要根据业务的需求来选择合适的字段类型。****

# 78.字段为什么要求定义为not null

1. 如果不设置NOT NULL的话，NULL是列的默认值，如果不是本身需要的话，尽量就不要使用NULL
2. 使用NULL带来更多的问题，比如索引、索引统计、值计算更加复杂，如果使用索引，就要避免列设置成NULL
3. 如果是索引列，会带来的存储空间的问题，需要额外的特殊处理，还会导致更多的存储空间占用
4. 对于稀疏数据又更好的空间效率，稀疏数据指的是**很多值为NULL，只有少数行的列有非NULL值**的情况

# 79.如果要存储用户的密码散列,应该使用什么字段进行存储?

密码散列,盐,用户身份证号等固定长度的字符串应该使用char而不是varchar来存储,这样可以 节省空间且提高检索效率。

# 80.Mysql驱动程序是什么？

PHP驱动程序

JDBC驱动程序

ODBC驱动程序

CWRAPPER

PYTHON驱动程序

 PERL驱动程序

RUBY驱动程序

CAP11PHP驱动程序

Ado.net5.mxj

# 81.MySQL中如何优化长难的查询语句？有实战过吗？

```js
将一个大的查询分为多个小的相同的查询
减少冗余记录的查询。
一个复杂查询可以考虑拆成多个简单查询
分解关联查询，让缓存的效率更高。
```

# 82.优化特定类型的查询语句

## 1、优化关联查询

优化关联查询，需要了解Mysql关联查询的执行逻辑，这点我们在mysql查询优化器 一文中有说明，想要了解的同学可以去看看，这里我们简单补充一下关联查询需要注意的几点：

确保on或using子句中的列上有索引。在创建索引的时候要考虑到关联的顺序，当表A和表B用列c关联的时候，如果优化器的关联顺序是B、A ，那么就不需要在B表的对应列上创建索引，只要在A上c列创建索引。除非有其他需要，否则只需要在关联顺序的第二个表的对应列上创建索引。多余的索引会只会带来额外的负担。
确保任何的GROUP BY 和 ORDER BY 中的表达式只涉及到一个表中的列，这样mysql才有可能使用所用来优化这个过程
升级Mysql时要注意：关联语法、运算符优先级等其他可能发生变化的地方。以前普通关联的地方可能会变成笛卡尔积，不同类型的关联可能会生成不同的结果。

## 2、优化子查询

关于子查询，大部分的优化策略都是尽可能的使用关联查询代替。但是我们在Mysql关联查询的局限性 中提到过，这不是绝对的，有时候关联子查询是合理的且性能是比较高的，要根据具体情况来定。不过大部分子查询的查询效率都比关联查询的效率低。

## 3、优化GROUP BY 和 DISTINCT 

大部分情况下，Mysql会使用同样的办法优化两种查询，且Mysql优化器会在内部处理的时候相互转化两类查询。他们都可以使用索引进行优化，且创建合理的所用也是最有效的办法。

当这两个语句在Mysql中无法使用索引时，GROUP BY 会使用两种策略来完成：使用临时表或文件排序来分组。这时可以通过使用提示SQL_BIG_RESULT和SQL_SMALL_RESULT 来让优化器对GROUP BY 或DISTINCT 查询如何使用临时表及排序。

select SQL_SMALL_RESULT gender from people GROUP BY gender
SQL_BIG_RESULT：告诉优化器结果集可能会非常大，建议使用磁盘临时表做排序操作
SQL_SMALL_RESULT：告诉优化器结果集会很小，可以将结果集放在内存中的索引临时表，以避免排序操作。
如果对关联查询做分组，并按照查找表的某个列进行分组，通常采用查找表的标识列分组的效率会比其他列高，如下面的查询效率就比较低：

```mysql
select actor.first_name , actor.last_name, count(*) from film_actor 
inner join actor using(actor_id)
group by actor.first_name, actor.last_name;
```

优化后的SQL:

```mysql
select actor.first_name , actor.last_name, count(*) from film_actor 
inner join actor using(actor_id)
group by film_actor.actor_id;
```

有时候关联语句的分组查询不能改写成select中直接使用非分组列行形式，甚至可能会在服务器上设置SQL_MODE来禁止这种写法。这时，我们可以通过使用min()、max()函数来绕过这种限制。 这里要注意一点：select 后面的非分组列，一定是依赖直接分组列的，且每个组内的值唯一，或者业务上不在乎具体值。

实际上SELECT中直接使用非分组列会有很多问题：这样的结果通常是不确定的，当索引改变或优化器策略改变都可能导致不同的结果。大多数这种查询最后都会出现故障，且Mysql不会对这类查询返回错误，而这种写法常常是因为程序员为了偷懒导致的。建议还是使用含义明确的语法，并将Mysql的SQL_MODE 设置为 ONLY_FULL_GROUP_BY。

## 4、优化LIMIT分页

优化分页的最简单的方法是，尽可能的使用覆盖索引，而不是查询所有的列。然后根据需要做一次关联操作返回需要的列，特别是对偏移量很大的时候，这种方法对效率提升极为明显。

### 5、优化UNION

Mysql总是通过创建并填充临时表的方式来执行UNION操作，所以很多优化策略在UNION查询中都没法很好使用。通常需要手动将where、limit、order by 等子句“下沉”到UNION的各个子查询中，进行优化。

除非确实需要服务器消除重复的行，否则一定要使用UNION ALL 来代替UNION。如果没有ALL关键字，MYSQL会给临时表加上DISTINCT 选项，这会导致对整个临时表的数据做唯一性检查，这样做的代价非常大。即便有ALL管家字，Mysql仍会使用临时表存储结果，再从临时表中读取数据返回给客户端。如果可以将数据直接返回给客户端，最好直接返回，不要存储在临时表。

# 83.MySQL数据库cpu飙升的话，要怎么处理呢？

```js
当 cpu 飙升到 100%时，先用操作系统命令 top 命令观察是不是 mysqld 占用导致的，
如果不是，找出占用高的进程，并进行相关处理。
如果是 mysqld 造成的， show processlist，看看里面跑的 session 情况，
是不是有消耗资源的 sql 在运行。找出消耗高的 sql，看看执行计划是否准确，
index 是否缺失，或者实在是数据量太大造成。
一般来说，肯定要 kill 掉这些线程(同时观察 cpu 使用率是否下降)，
等进行相应的调整(比如说加索引、改 sql、改内存参数)之后，再重新跑这些 SQL。
也有可能是每个 sql 消耗资源并不多，但是突然之间，有大量的 session 连进来导致 cpu 飙升，
这种情况就需要跟应用一起来分析为何连接数会激增，
再做出相应的调整，比如说限制连接数等
```

# 84.读写分离常见方案？

【常见方案】
方案1：应用程序根据业务逻辑来判断，增删改等写操作命令发给主库，查询命令发给备库。
特点：<1>数据库和应用程序强耦合，数据库如果有变化还好影响主库。<2>应用程序复杂化。

方案2：利用中间件来做代理，负责对数据库的请求识别出读还是写，并分发到不同的数据库中。
特点：<1> 数据库和应用程序弱耦合。<2> 代理存在性能瓶颈和可靠性风险增加，相对可控。
常见的中间件：<1> MySQL-Proxy  <2> Amoeba for MySQL(原文推荐这个，没机会对比过)  <3> mycat  <4> DBProxy  <5> 公有云的RDS数据库+数据库中间件，如华为云的RDS(关系型数据库)+DDM(分布式数据库中间件)

方案3：mysql集群模式，罕见，复杂度高，稳定性差。


# 85.MySQL的复制原理以及流程

1、Master将数据改变记录到二进制日志(binary log)中，也就是配置文件log-bin指定的文件，这些记录叫做二进制日志事件(binary log events)
 2、Slave通过I/O线程读取Master中的binary log events并写入到它的中继日志(relay log)
 3、Slave重做中继日志中的事件，把中继日志中的事件信息一条一条的在本地执行一次，完成数据在本地的存储，从而实现将改变反映到它自己的数据(数据重放)

## 二进制日志转储线程 （Binlog dump thread）

是一个主库线程。当从库线程连接的时候， 主库可以将二进制日志发送给从库，当主库读取事件（Event）的时候，

会在 Binlog 上 加锁 ，读取完成之后，再将锁释放掉。

## 从库 I/O 线程 

会连接到主库，向主库发送请求更新 Binlog。这时从库的 I/O 线程就可以读取到主库的

二进制日志转储线程发送的 Binlog 更新部分，并且拷贝到本地的中继日志 （Relay log）。

## 从库 SQL 线程 

会读取从库中的中继日志，并且执行日志中的事件，将从库中的数据与主库保持同步。

# 86.datetime和timestamp的区别

## 相同点

- 存储格式相同 datetime和timestamp两者的时间格式都是YYYY-MM-DD HH:MM:SS

## 不同点

- 存储范围不同. datetime的范围是1000-01-01到9999-12-31. 而timestamp是从1970-01-01到2038-01-19, 即后者的时间范围很小.
- 与时区关系. datetime是存储服务器当前的时区. 而timestamp类型,是将服务器当前时间转换为UTC(世界时间)来存储.即datetime与时区无关,存什么,返回什么. 而timestamp存储的时间,返回的时间会随着数据库的时区不同而发生改变.

## 3 总结

关于datetime和timestamp的选择使用.

- 1 在满足使用条件的情况下,占据存储空间越少越好,此时选择timestamp比datetime更好.
- 2 timestamp类型的存储,是以UTC时区来保存的,在显示时会自动将日期数据转换,如果时间的场景对应多个时区,此时选择timestamp比datetime更好.
- 3 关于日期的使用范围, timestamp类型最大只支持到2038-01-19年,所以如果使用的日期时间较大, 则选择datetime比timestamp更好.

# 87.MySQL TEXT数据类型的最大长度

| TINYTEXT   | 256 bytes           |       |
| ---------- | ------------------- | ----- |
| TEXT       | 65,535 bytes        | ~64kb |
| MEDIUMTEXT | 16,777,215 bytes    | ~16MB |
| LONGTEXT   | 4,294,967,295 bytes | ~4GB  |

# 88.谈谈关于MySQL explain 的详解

通过explain我们可以知道：表的读取顺序，数据读取操作的类型，哪些索引可以使用，哪些索引实际使用了等等。

# 89.500台db，在最快时间之内重启，怎么实现？

可以使用批量 ssh 工具 pssh 来对需要重启的机器执行重启命令。

也可以使用 salt（前提是客户端有安装 salt）或者 ansible（ ansible 只需要 ssh 免登通了就行）等多线程工具同时操作多台服务

# 90.MySQL 你是如何监控你们的数据库的？你们的慢日志都是怎么查询的？

监控的工具有很多，例如zabbix，lepus，我这里用的是lepus

通过开启慢查询日志，设定慢查询所界定的查询时间，使用mysqlslowdump进行慢查询日志分析

# 91.你是否做过主从一致性校验，如果有，怎么做的，如果没有，你打算怎么做？

**pt-table-checksum**

pt-table-checksum的工作原理很简单，通过在主库和从库对表进行一致性的checksum计算，如果主从库的某张表（结构或者数据）不一致，那么checksum结果值就不一样，通过对比主库和从库checksum的值就可以很容易的知道主从是否数据一致了（checksum结果见下文示例）。

# 92.你们数据库是否支持emoji表情存储，如果不支持，如何操作？

更换字符集utf8-->utf8mb4

# 93.MySQL如何获取当前日期？

 NOW() 和 SYSDATE() 函数

# 94.一个6亿的表a，一个3亿的表b，通过外间tid关联，如何最快的查询出满足条件的第50000到第50200中的这200条数据记录？

1、如果A表TID是自增长,并且是连续的,B表的ID为索引

select * from a,b where a.tid = b.id and a.tid>500000 limit 200;

2、如果A表的TID不是连续的,那么就需要使用覆盖索引.TID要么是主键,要么是辅助索引,B表ID也需要有索引。select * from b , (select tid from a limit 50000,200) a where b.id = a .tid;

# 95.Mysql一条SQL加锁分析

## 组合一：id主键+RC

这个组合，是最简单，最容易分析的组合。id是主键，Read Committed隔离级别，给定SQL：delete from t1 where id = 10; 只需要将主键上，id = 10的记录加上X锁即可。SELECT不加锁。

## 组合二：id唯一索引+RC

id不是主键，是一个Unique的二级索引键值，而主键是其他列。加锁的情况由于组合一有所不同。由于id是unique索引，因此delete语句会选择走id列的索引进行where条件的过滤，在找到id=10的记录后，首先会将unique索引上的id=10索引记录加上X锁，同时，会根据读取到的主键列，回主键索引(聚簇索引)，然后将聚簇索引上的主键列对应的索引项加X锁。

为什么聚簇索引上的记录也要加锁？试想一下，如果并发的一个SQL，是通过主键索引来更新：update t1 set id = 100 where PRIMARY KEY = 'xxx’; 此时，如果delete语句没有将主键索引上的记录加锁，那么并发的update就会感知不到delete语句的存在，违背了同一记录上的更新/删除需要串行执行的约束。

若id列是unique列，其上有unique索引。那么SQL需要加两个X锁，一个对应于id unique索引上的id = 10的记录，另一把锁对应于聚簇索引上的数据。

## 组合三：id非唯一索引+RC

与组合二很类似，区别在于id列上的约束又降低了，id列不再唯一，只有一个普通的索引。假设delete from t1 where id = 10; 语句，仍旧选择id列上的索引进行过滤where条件，那么此时首先，id列索引上，满足id = 10查询条件的记录，均已加锁。同时，这些记录对应的主键索引上的记录也都加上了锁。

组合二最多只有一个满足等值查询的记录，而组合三会将所有满足查询条件的记录都加锁。

## 组合四：id无索引+RC

当id列上没有索引，where id = 10;这个过滤条件，没法通过索引进行过滤，因此只能走聚簇索引，那么只能走全表扫描做过滤。

当全表扫描时，即使满足删除条件的记录有两条，但是，聚簇索引上所有的记录，都被加上了X锁。无论记录是否满足条件，全部被加上X锁。既不是加表锁，也不是在满足条件的记录上加行锁。

MySQL的实现决定了。如果一个条件无法通过索引快速过滤，那么存储引擎层面就会将所有记录加锁后返回，然后由MySQL Server层进行过滤。因此也就把所有的记录，都锁上了。

在实际的实现中，MySQL有一些改进，在MySQL Server过滤条件，发现不满足后，会调用unlock_row方法，把不满足条件的记录放锁。保证了最后只会持有满足条件记录上的锁，但是每条记录的加锁操作还是不能省略的。

结论：若id列上没有索引，SQL会走聚簇索引的全扫描进行过滤，由于过滤是由MySQL Server层面进行的。因此每条记录，无论是否满足条件，都会被加上X锁。但是，为了效率考量，MySQL做了优化，对于不满足条件的记录，会在判断后放锁，最终持有的，是满足条件的记录上的锁。

## 组合五：id主键+RR

id列是主键列，RR隔离级别，针对delete from t1 where id = 10; 这条SQL，加锁与RC隔离级别一致。

## 组合六：id唯一索引+RR

与RC隔离级别一致。两个X锁，id唯一索引满足条件的记录上一个，对应的聚簇索引上的记录一个。

## 组合七：id非唯一索引+RR

前文提到，RC隔离级别允许幻读，而RR隔离级别，不允许存在幻读。但是在组合五、组合六中，加锁行为又是与RC下的加锁行为完全一致。那么RR隔离级别下，id上有一个非唯一索引，执行delete from t1 where id = 10; 会比在RC隔离级别多一个GAP锁。

假设 t1 表中，name是主键PRIMARY KEY。

GAP锁的目的，就是RR隔离级别，相对于RC隔离级别，不会出现幻读的关键，是为了防止同一事务的两次当前读，出现幻读的情况。GAP锁锁住的位置，也不是记录本身，而是两条记录之间的位置。

要保证两次当前读返回一致的记录，那就需要在第一次当前读与第二次当前读之间，其他的事务不会插入新的满足条件的记录并提交。

如图中所示，为了保证[6,c]与[10,b]间，[10,b]与[10,d]间，[10,d]与[11,f]不会插入新的满足条件的记录，MySQL选择了用GAP锁，将这三个GAP给锁起来。因此在其他连接执行插入前，会检查这个GAP是否已经被锁上，如果被锁上，则Insert不能插入记录。因此，通过第一遍的当前读，不仅将满足条件的记录锁上 (X锁)，与组合三类似。同时还是增加3把GAP锁，将可能插入满足条件记录的3个GAP给锁上，保证后续的Insert不能插入新的id=10的记录，也就杜绝了同一事务的第二次当前读，出现幻象的情况。

## 组合八：id无索引+RR

RR隔离级别下，id列上没有索引。此时SQL：delete from t1 where id = 10; 没有其他的路径可以选择，只能进行全表扫描。最终的加锁是一个很恐怖的现象。首先，聚簇索引上的所有记录，都被加上了X锁。其次，聚簇索引每条记录间的间隙(GAP)，也同时被加上了GAP锁。

在这种情况下，这个表上，除了不加锁的快照度，其他任何加锁的并发SQL，均不能执行，不能更新，不能删除，不能插入，全表被锁死。

MySQL针对这种情况也做了一些优化，就是所谓的semi-consistent read。semi-consistent read开启的情况下，对于不满足查询条件的记录，MySQL会提前放锁。针对上面的这个用例，就是除了记录[d,10]，[g,10]之外，所有的记录锁都会被释放，同时不加GAP锁。关于semi-consistent read的详情本文不介绍。

## 组合九：Serializable

Serializable隔离级别。对于SQL2：delete from t1 where id = 10; 来说，Serializable隔离级别与Repeatable Read隔离级别完全一致。

Serializable隔离级别，影响的是SQL1：select * from t1 where id = 10; 这条SQL，在RC，RR隔离级别下，都是快照读，不加锁。但是在Serializable隔离级别，SQL1会加读锁，也就是说快照读不复存在，MVCC并发控制降级为Lock-Based CC。

## 复杂SQL的加锁分析

到这里MySQL的加锁实现也已经介绍的八八九九。只要将本文上面的分析思路，大部分的SQL，都能分析出其会加哪些锁。而这里，再来看一个稍微复杂点的SQL，用于说明MySQL加锁的另外一个逻辑。表结构和SQL用例如下：

```mysql
CREATE TABLE `t1` (
  `id` char(32) NOT NULL COMMENT 'id',
  `user_id` varchar(5) NOT NULL DEFAULT '' ,
  `blog_id` varchar(5) NOT NULL DEFAULT '',
  `pubtime` int(10) DEFAULT NULL,
  `comment` varchar(255) NOT NULL DEFAULT '' ,
  PRIMARY KEY (`id`),
  KEY `idx_t1_pu` (`pubtime`,`pubtime`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
```

在上述表结构下，执行如下SQL

```mysql
delete  from t1 where pubtime>1 and pubtime<20 and userid = 'hbc' and comment is not null;
假定在默认的Repeatable Read隔离级别下。
在详细分析这条SQL的加锁情况前，还需要有一个知识储备，那就是一个SQL中的where条件如何拆分？笔者不过多介绍，直接说结论。
```

Index key：pubtime > 1 and puptime < 20。此条件，用于确定SQL在idx_t1_pu索引上的查询范围。
Index Filter：userid = ‘hdc’ 。此条件，可以在idx_t1_pu索引上进行过滤，但不属于Index Key。
Table Filter：comment is not NULL。此条件，在idx_t1_pu索引上无法过滤，只能在聚簇索引上过滤。
在分析出SQL where条件的构成之后，再来看看这条SQL的加锁情况 (RR隔离级别)，如下图所示：



在RR隔离级别下，由Index Key所确定的范围，被加上了GAP锁；Index Filter锁给定的条件 (userid = ‘hdc’)何时过滤，视MySQL的版本而定，在MySQL 5.6版本之前，不支持Index Condition Pushdown(ICP)，因此Index Filter在MySQL Server层过滤，在5.6后支持了Index Condition Pushdown，则在index上过滤。

若不支持ICP，不满足Index Filter的记录，也需要加上记录X锁，若支持ICP，则不满足Index Filter的记录，无需加记录X锁 (图中，用红色箭头标出的X锁，是否要加，视是否支持ICP而定)；

而Table Filter对应的过滤条件，则在聚簇索引中读取后，在MySQL Server层面过滤，因此聚簇索引上也需要X锁。最后，选取出了一条满足条件的记录[8,hdc,d,5,good]，但是加锁的数量，要远远大于满足条件的记录数量。

结论：在Repeatable Read隔离级别下，针对一个复杂的SQL，首先需要提取其where条件。Index Key确定的范围，需要加上GAP锁；Index Filter过滤条件，视MySQL版本是否支持ICP，若支持ICP，则不满足Index Filter的记录，不加X锁，否则需要X锁；Table Filter过滤条件，无论是否满足，都需要加X锁。

六、总结
要做的完全掌握MySQL/InnoDB的加锁规则，甚至是其他任何数据库的加锁规则，需要具备以下的一些知识点：

了解数据库的一些基本理论知识：

数据的存储格式 (堆组织表 vs 聚簇索引表)；并发控制协议 (MVCC vs Lock-Based CC)；Two-Phase Locking；数据库的隔离级别定义 (Isolation Level)；
了解SQL本身的执行计划 (主键扫描 vs 唯一键扫描 vs 范围扫描 vs 全表扫描)；
了解数据库本身的一些实现细节 (过滤条件提取；Index Condition Pushdown；Semi-Consistent Read)；
了解死锁产生的原因及分析的方法 (加锁顺序不一致；分析每个SQL的加锁顺序)

# 96.InnoDB存储引擎的锁的算法有三种

- Record Lock：记录**锁**
- Gap Lock：间隙**锁**
- Next-Key Lock：临键**锁**

# 97.sql的语句的执行顺序

## 1、from 

子句组装来自不同数据源的数据；

##  2、where 

子句基于指定的条件对记录行进行筛选；

##   3、group by 

子句将数据划分为多个分组；

##   4、聚集函数

使用聚集函数进行计算；

##   5、having

使用 having 子句筛选分组；

##   6、表达式

计算所有的表达式；

##   7、select 的字段

select 的字段；

##   8、 order by 

使用 order by 对结果集进行排序。
